{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install ipywidgets'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install deepspeech'''\n",
    "'''!pip install ipywidgets'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import scipy\n",
    "import matplotlib as plt\n",
    "import soundfile as sf\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from textgrids import TextGrid\n",
    "from numba import jit\n",
    "from typing import Optional\n",
    "\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import librosa\n",
    "import jiwer\n",
    "import logging\n",
    "import deepspeech\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm\n",
    "from torch.nn import Conv1d, Conv2d, ConvTranspose1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import flags\n",
    "import sys\n",
    "\n",
    "#FLAGS = flags.FLAGS\n",
    "'''\n",
    "# Define flags before parsing\n",
    "flags.DEFINE_list('remove_channels', [], 'channels to remove')\n",
    "flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')\n",
    "flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data', './emg_data/nonparallel_data'], 'voiced data locations')\n",
    "flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')\n",
    "flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')\n",
    "'''\n",
    "'''# Modify sys.argv to avoid issues with Jupyter's own arguments\n",
    "argv = sys.argv[:1]  # Keep only the first argument (the script/notebook itself)\n",
    "FLAGS(argv)  # Parse flags with modified argv'''\n",
    "\n",
    "\n",
    "remove_channels = []\n",
    "silent_data_directories = ['./emg_data/silent_parallel_data']\n",
    "voiced_data_directories = ['./emg_data/voiced_parallel_data', './emg_data/nonparallel_data']\n",
    "testset_file = 'testset_largedev.json'\n",
    "text_align_directory = 'text_alignments'\n",
    "normalizers_file = 'normalizers.pkl'\n",
    "learning_rate = 0.001 \n",
    "output_directory = './output' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering: double average smooths the signal, get_emg_features extracts the manual features. Section 3.1.2 of the disertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_average(x):\n",
    "    assert len(x.shape) == 1\n",
    "    f = np.ones(9)/9.0\n",
    "    v = np.convolve(x, f, mode='same')\n",
    "    w = np.convolve(v, f, mode='same')\n",
    "    return w\n",
    "\n",
    "def get_emg_features(emg_data, debug=False):\n",
    "    xs = emg_data - emg_data.mean(axis=0, keepdims=True)\n",
    "    frame_features = []\n",
    "    for i in range(emg_data.shape[1]):\n",
    "        x = xs[:,i]\n",
    "        w = double_average(x)\n",
    "        p = x - w\n",
    "        r = np.abs(p)\n",
    "\n",
    "        w_h = librosa.util.frame(w, frame_length=16, hop_length=6).mean(axis=0)\n",
    "        p_w = librosa.feature.rms(y=w, frame_length=16, hop_length=6, center=False)\n",
    "        p_w = np.squeeze(p_w, 0)\n",
    "        p_r = librosa.feature.rms(y=r, frame_length=16, hop_length=6, center=False)\n",
    "        p_r = np.squeeze(p_r, 0)\n",
    "        z_p = librosa.feature.zero_crossing_rate(p, frame_length=16, hop_length=6, center=False)\n",
    "        z_p = np.squeeze(z_p, 0)\n",
    "        r_h = librosa.util.frame(r, frame_length=16, hop_length=6).mean(axis=0)\n",
    "\n",
    "        s = abs(librosa.stft(np.ascontiguousarray(x), n_fft=16, hop_length=6, center=False))\n",
    "        # s has feature dimension first and time second\n",
    "\n",
    "        if debug:\n",
    "            plt.subplot(7,1,1)\n",
    "            plt.plot(x)\n",
    "            plt.subplot(7,1,2)\n",
    "            plt.plot(w_h)\n",
    "            plt.subplot(7,1,3)\n",
    "            plt.plot(p_w)\n",
    "            plt.subplot(7,1,4)\n",
    "            plt.plot(p_r)\n",
    "            plt.subplot(7,1,5)\n",
    "            plt.plot(z_p)\n",
    "            plt.subplot(7,1,6)\n",
    "            plt.plot(r_h)\n",
    "\n",
    "            plt.subplot(7,1,7)\n",
    "            plt.imshow(s, origin='lower', aspect='auto', interpolation='nearest')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        frame_features.append(np.stack([w_h, p_w, p_r, z_p, r_h], axis=1))\n",
    "        frame_features.append(s.T)\n",
    "\n",
    "    frame_features = np.concatenate(frame_features, axis=1)\n",
    "    return frame_features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize volume adjusts root mean squared amplitude, and scale it. synamic_range_compression_torch applies a log transform. Spectral_normalize_torch looks very redundant. mel_spectogram creates the mel spectogram from audio.  \n",
    "  \n",
    "load_audio takes in the audiofile, extracts audio wave and sample rate, slice it based on start and end, normalize volume, resample to 22,050Hz, converts it to a mel-spectogram ith the properties: 1024 size of FFT window, 80 mel bands, 22050 sampling rate, 256 and 1024 stride and window for STFT, frequency range 0 - 8000Hz.  \n",
    "Then it truncates the number of frames to max_frames and return the mel-spectogram where each row is a time frame and each column is a frequency bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_volume(audio):\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    max_rms = rms.max() + 0.01\n",
    "    target_rms = 0.2\n",
    "    audio = audio * (target_rms/max_rms)\n",
    "    max_val = np.abs(audio).max()\n",
    "    if max_val > 1.0: # this shouldn't happen too often with the target_rms of 0.2\n",
    "        audio = audio / max_val\n",
    "    return audio\n",
    "\n",
    "\n",
    "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "def spectral_normalize_torch(magnitudes):\n",
    "    output = dynamic_range_compression_torch(magnitudes)\n",
    "    return output\n",
    "\n",
    "mel_basis = {}\n",
    "hann_window = {}\n",
    "\n",
    "\n",
    "def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "\n",
    "    global mel_basis, hann_window\n",
    "    if fmax not in mel_basis:\n",
    "        mel = librosa.filters.mel(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
    "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
    "\n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
    "    spec = torch.view_as_real(spec)\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
    "\n",
    "    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)\n",
    "    spec = spectral_normalize_torch(spec)\n",
    "\n",
    "    return spec\n",
    "\n",
    "def load_audio(filename, start=None, end=None, max_frames=None, renormalize_volume=False):\n",
    "    audio, r = sf.read(filename)\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio[:,0] # select first channel of stero audio\n",
    "    if start is not None or end is not None:\n",
    "        audio = audio[start:end]\n",
    "\n",
    "    if renormalize_volume:\n",
    "        audio = normalize_volume(audio)\n",
    "    if r == 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=16000, target_sr=22050)\n",
    "    else:\n",
    "        assert r == 22050\n",
    "    audio = np.clip(audio, -1, 1) # because resampling sometimes pushes things out of range\n",
    "    pytorch_mspec = mel_spectrogram(torch.tensor(audio, dtype=torch.float32).unsqueeze(0), 1024, 80, 22050, 256, 1024, 0, 8000, center=False)\n",
    "    mspec = pytorch_mspec.squeeze(0).T.numpy()\n",
    "    if max_frames is not None and mspec.shape[0] > max_frames:\n",
    "        mspec = mspec[:max_frames,:]\n",
    "    return mspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_phonemes create the phoneme alignment using Montreal Forced Aligner, clean up the data, then return an array of phonemes, one for each frame. Section 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_phonemes(textgrid_fname, max_len=None):\n",
    "    tg = TextGrid(textgrid_fname)\n",
    "    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)\n",
    "    phone_ids[:] = -1\n",
    "    phone_ids[-1] = phoneme_inventory.index('sil') # make sure list is long enough to cover full length of original sequence\n",
    "    for interval in tg['phones']:\n",
    "        phone = interval.text.lower()\n",
    "        if phone in ['', 'sp', 'spn']:\n",
    "            phone = 'sil'\n",
    "        if phone[-1] in string.digits:\n",
    "            phone = phone[:-1]\n",
    "        ph_id = phoneme_inventory.index(phone)\n",
    "        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id\n",
    "    assert (phone_ids >= 0).all(), 'missing aligned phones'\n",
    "\n",
    "    if max_len is not None:\n",
    "        phone_ids = phone_ids[:max_len]\n",
    "        assert phone_ids.shape[0] == max_len\n",
    "    return phone_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove_drift applies a 2 Hz high pass filter, section 3.1, applies to both manual and learned features.  \n",
    "notch is used to remove the AC electrical noise at 60hz, section 3.1.  \n",
    "notch harmonics just iteratively apply notch.  \n",
    "subsample downsamples the EMG signal to a new rate  \n",
    "apply_to_all just applies an input function to each column  \n",
    "  \n",
    "load_utterance takes in a raw emg file, applies the remove drift, notch and subsample. It then gets the manual features. Then it gets the mfcc from the corresponding audio file. If emg_features have more frames than mfccs, emg_features gets truncated. The transcription gets loaded and phonemes are aligned with audio using the read_phonemes function. The function returns: mfccs, emg_features (manual), text transcription, book location, array of phonemes, original emg data but with the common transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_drift(signal, fs):\n",
    "    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def notch(signal, freq, sample_frequency):\n",
    "    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def notch_harmonics(signal, freq, sample_frequency):\n",
    "    for harmonic in range(1,8):\n",
    "        signal = notch(signal, freq*harmonic, sample_frequency)\n",
    "    return signal\n",
    "\n",
    "def subsample(signal, new_freq, old_freq):\n",
    "    times = np.arange(len(signal))/old_freq\n",
    "    sample_times = np.arange(0, times[-1], 1/new_freq)\n",
    "    result = np.interp(sample_times, times, signal)\n",
    "    return result\n",
    "\n",
    "def apply_to_all(function, signal_array, *args, **kwargs):\n",
    "    results = []\n",
    "    for i in range(signal_array.shape[1]):\n",
    "        results.append(function(signal_array[:,i], *args, **kwargs))\n",
    "    return np.stack(results, 1)\n",
    "\n",
    "def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):\n",
    "    index = int(index)\n",
    "    raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))\n",
    "    before = os.path.join(base_dir, f'{index-1}_emg.npy')\n",
    "    after = os.path.join(base_dir, f'{index+1}_emg.npy')\n",
    "    if os.path.exists(before):\n",
    "        raw_emg_before = np.load(before)\n",
    "    else:\n",
    "        raw_emg_before = np.zeros([0,raw_emg.shape[1]])\n",
    "    if os.path.exists(after):\n",
    "        raw_emg_after = np.load(after)\n",
    "    else:\n",
    "        raw_emg_after = np.zeros([0,raw_emg.shape[1]])\n",
    "\n",
    "    x = np.concatenate([raw_emg_before, raw_emg, raw_emg_after], 0)\n",
    "    x = apply_to_all(notch_harmonics, x, 60, 1000)\n",
    "    x = apply_to_all(remove_drift, x, 1000)\n",
    "    x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]\n",
    "    emg_orig = apply_to_all(subsample, x, 689.06, 1000)\n",
    "    x = apply_to_all(subsample, x, 516.79, 1000)\n",
    "    emg = x\n",
    "\n",
    "    for c in remove_channels:\n",
    "        emg[:,int(c)] = 0\n",
    "        emg_orig[:,int(c)] = 0\n",
    "\n",
    "    emg_features = get_emg_features(emg)\n",
    "\n",
    "    mfccs = load_audio(os.path.join(base_dir, f'{index}_audio_clean.flac'),\n",
    "            max_frames=min(emg_features.shape[0], 800 if limit_length else float('inf')))\n",
    "\n",
    "    if emg_features.shape[0] > mfccs.shape[0]:\n",
    "        emg_features = emg_features[:mfccs.shape[0],:]\n",
    "    assert emg_features.shape[0] == mfccs.shape[0]\n",
    "    emg = emg[6:6+6*emg_features.shape[0],:]\n",
    "    emg_orig = emg_orig[8:8+8*emg_features.shape[0],:]\n",
    "    assert emg.shape[0] == emg_features.shape[0]*6\n",
    "\n",
    "    with open(os.path.join(base_dir, f'{index}_info.json')) as f:\n",
    "        info = json.load(f)\n",
    "\n",
    "    sess = os.path.basename(base_dir)\n",
    "    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'\n",
    "    if os.path.exists(tg_fname):\n",
    "        phonemes = read_phonemes(tg_fname, mfccs.shape[0])\n",
    "    else:\n",
    "        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')\n",
    "        \n",
    "    #print(mfccs)\n",
    "\n",
    "    return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGDirectory(object):\n",
    "    def __init__(self, session_index, directory, silent, exclude_from_testset=False):\n",
    "        self.session_index = session_index\n",
    "        self.directory = directory\n",
    "        self.silent = silent\n",
    "        self.exclude_from_testset = exclude_from_testset\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.session_index < other.session_index\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleans text and can convert it to an integer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform(object):\n",
    "    def __init__(self):\n",
    "        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "        self.chars = string.ascii_lowercase+string.digits+' '\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = unidecode(text)\n",
    "        text = self.transformation(text)\n",
    "        return text\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        return [self.chars.index(c) for c in text]\n",
    "\n",
    "    def int_to_text(self, ints):\n",
    "        return ''.join(self.chars[i] for i in ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init: sets up directories, extracts examples of mfccs and emg_features  \n",
    "subset: returns a subset of the data  \n",
    "silent_subset: returns a subset of data containing only silent examples  \n",
    "  \n",
    "getitem: loads the data from load_utterance (mfccs, emg, text, book_location, phonemes, raw_emg). Does despiking by passing raw emg through tanh. Normalize mfccs and emg features, gets the session id, audiofile and integer representation of text. Puts audio_features, manual emg features,text, integer representation of text, file index, session_ids, book_location, silent directory and raw_emg into the results dict. If it is silent speech, it also adds the parallel audio features (mfccs) and parallel EMG (manual features from voiced emg)  \n",
    "  \n",
    "Collate organize some data and returns\n",
    "                {audio_features, audio_feature_lengths, emg, raw_emg, parallel_voiced_emg, phonemes, session_ids, lengths, silent,text_ints, text_int_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):\n",
    "\n",
    "        self.text_align_directory = text_align_directory\n",
    "\n",
    "        if no_testset:\n",
    "            devset = []\n",
    "            testset = []\n",
    "        else:\n",
    "            with open(testset_file) as f:\n",
    "                testset_json = json.load(f)\n",
    "                devset = testset_json['dev']\n",
    "                testset = testset_json['test']\n",
    "\n",
    "        directories = []\n",
    "        if base_dir is not None:\n",
    "            directories.append(EMGDirectory(0, base_dir, False))\n",
    "        else:\n",
    "            for sd in silent_data_directories:\n",
    "                for session_dir in sorted(os.listdir(sd)):\n",
    "                    directories.append(EMGDirectory(len(directories), os.path.join(sd, session_dir), True))\n",
    "\n",
    "            has_silent = len(silent_data_directories) > 0\n",
    "            for vd in voiced_data_directories:\n",
    "                for session_dir in sorted(os.listdir(vd)):\n",
    "                    directories.append(EMGDirectory(len(directories), os.path.join(vd, session_dir), False, exclude_from_testset=has_silent))\n",
    "\n",
    "        self.example_indices = []\n",
    "        self.voiced_data_locations = {} # map from book/sentence_index to directory_info/index\n",
    "        for directory_info in directories:\n",
    "            for fname in os.listdir(directory_info.directory):\n",
    "                m = re.match(r'(\\d+)_info.json', fname)\n",
    "                if m is not None:\n",
    "                    idx_str = m.group(1)\n",
    "                    with open(os.path.join(directory_info.directory, fname)) as f:\n",
    "                        info = json.load(f)\n",
    "                        if info['sentence_index'] >= 0: # boundary clips of silence are marked -1\n",
    "                            location_in_testset = [info['book'], info['sentence_index']] in testset\n",
    "                            location_in_devset = [info['book'], info['sentence_index']] in devset\n",
    "                            if (test and location_in_testset and not directory_info.exclude_from_testset) \\\n",
    "                                    or (dev and location_in_devset and not directory_info.exclude_from_testset) \\\n",
    "                                    or (not test and not dev and not location_in_testset and not location_in_devset):\n",
    "                                self.example_indices.append((directory_info,int(idx_str)))\n",
    "\n",
    "                            if not directory_info.silent:\n",
    "                                location = (info['book'], info['sentence_index'])\n",
    "                                self.voiced_data_locations[location] = (directory_info,int(idx_str))\n",
    "\n",
    "        self.example_indices.sort()\n",
    "        random.seed(0)\n",
    "        random.shuffle(self.example_indices)\n",
    "\n",
    "        self.no_normalizers = no_normalizers\n",
    "        if not self.no_normalizers:\n",
    "            self.mfcc_norm, self.emg_norm = pickle.load(open('normalizers.pkl','rb'))\n",
    "\n",
    "        sample_mfccs, sample_emg, _, _, _, _ = load_utterance(self.example_indices[0][0].directory, self.example_indices[0][1])\n",
    "        self.num_speech_features = sample_mfccs.shape[1]\n",
    "        self.num_features = sample_emg.shape[1]\n",
    "        self.limit_length = limit_length\n",
    "        self.num_sessions = len(directories)\n",
    "\n",
    "        self.text_transform = TextTransform()\n",
    "        \n",
    "    def silent_subset(self):\n",
    "        result = copy(self)\n",
    "        silent_indices = []\n",
    "        for example in self.example_indices:\n",
    "            if example[0].silent:\n",
    "                silent_indices.append(example)\n",
    "        result.example_indices = silent_indices\n",
    "        return result\n",
    "\n",
    "    def subset(self, fraction):\n",
    "        result = copy(self)\n",
    "        result.example_indices = self.example_indices[:int(fraction*len(self.example_indices))]\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.example_indices)\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, i):\n",
    "        directory_info, idx = self.example_indices[i]\n",
    "        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)\n",
    "        raw_emg = raw_emg / 20\n",
    "        raw_emg = 50*np.tanh(raw_emg/50.)\n",
    "\n",
    "        if not self.no_normalizers:\n",
    "            mfccs = self.mfcc_norm.normalize(mfccs)\n",
    "            emg = self.emg_norm.normalize(emg)\n",
    "            emg = 8*np.tanh(emg/8.)\n",
    "\n",
    "        session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)\n",
    "        audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'\n",
    "\n",
    "        text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)\n",
    "\n",
    "        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}\n",
    "\n",
    "        if directory_info.silent:\n",
    "            voiced_directory, voiced_idx = self.voiced_data_locations[book_location]\n",
    "            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)\n",
    "\n",
    "            if not self.no_normalizers:\n",
    "                voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)\n",
    "                voiced_emg = self.emg_norm.normalize(voiced_emg)\n",
    "                voiced_emg = 8*np.tanh(voiced_emg/8.)\n",
    "\n",
    "            result['parallel_voiced_audio_features'] = torch.from_numpy(voiced_mfccs).pin_memory()\n",
    "            result['parallel_voiced_emg'] = torch.from_numpy(voiced_emg).pin_memory()\n",
    "\n",
    "            audio_file = f'{voiced_directory.directory}/{voiced_idx}_audio_clean.flac'\n",
    "\n",
    "        result['phonemes'] = torch.from_numpy(phonemes).pin_memory() # either from this example if vocalized or aligned example if silent\n",
    "        result['audio_file'] = audio_file\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_raw(batch):\n",
    "        batch_size = len(batch)\n",
    "        audio_features = []\n",
    "        audio_feature_lengths = []\n",
    "        parallel_emg = []\n",
    "        for ex in batch:\n",
    "            if ex['silent']:\n",
    "                audio_features.append(ex['parallel_voiced_audio_features'])\n",
    "                audio_feature_lengths.append(ex['parallel_voiced_audio_features'].shape[0])\n",
    "                parallel_emg.append(ex['parallel_voiced_emg'])\n",
    "            else:\n",
    "                audio_features.append(ex['audio_features'])\n",
    "                audio_feature_lengths.append(ex['audio_features'].shape[0])\n",
    "                parallel_emg.append(np.zeros(1))\n",
    "        phonemes = [ex['phonemes'] for ex in batch]\n",
    "        emg = [ex['emg'] for ex in batch]\n",
    "        raw_emg = [ex['raw_emg'] for ex in batch]\n",
    "        session_ids = [ex['session_ids'] for ex in batch]\n",
    "        lengths = [ex['emg'].shape[0] for ex in batch]\n",
    "        silent = [ex['silent'] for ex in batch]\n",
    "        text_ints = [ex['text_int'] for ex in batch]\n",
    "        text_lengths = [ex['text_int'].shape[0] for ex in batch]\n",
    "\n",
    "        result = {'audio_features':audio_features,\n",
    "                  'audio_feature_lengths':audio_feature_lengths,\n",
    "                  'emg':emg,\n",
    "                  'raw_emg':raw_emg,\n",
    "                  'parallel_voiced_emg':parallel_emg,\n",
    "                  'phonemes':phonemes,\n",
    "                  'session_ids':session_ids,\n",
    "                  'lengths':lengths,\n",
    "                  'silent':silent,\n",
    "                  'text_int':text_ints,\n",
    "                  'text_int_lengths':text_lengths}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LearnedRelativePositionalEmbedding(nn.Module):\n",
    "    # from https://github.com/pytorch/fairseq/pull/2225/commits/a7fb63f2b84d5b20c8855e9c3372a95e5d0ea073\n",
    "    \"\"\"\n",
    "    This module learns relative positional embeddings up to a fixed\n",
    "    maximum size. These are masked for decoder and unmasked for encoder\n",
    "    self attention.\n",
    "    By default the embeddings are added to keys, but could be added to\n",
    "    values as well.\n",
    "    Args:\n",
    "        max_relative_pos (int): the maximum relative positions to compute embeddings for\n",
    "        num_heads (int): number of attention heads\n",
    "        embedding_dim (int): depth of embeddings\n",
    "        unmasked (bool): if the attention is unmasked (for transformer encoder)\n",
    "        heads_share_embeddings (bool): if heads share the same relative positional embeddings\n",
    "        add_to_values (bool): compute embeddings to be added to values as well\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_relative_pos: int,\n",
    "            num_heads: int,\n",
    "            embedding_dim: int,\n",
    "            unmasked: bool = False,\n",
    "            heads_share_embeddings: bool = False,\n",
    "            add_to_values: bool = False):\n",
    "        super().__init__()\n",
    "        self.max_relative_pos = max_relative_pos\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.unmasked = unmasked\n",
    "        self.heads_share_embeddings = heads_share_embeddings\n",
    "        self.add_to_values = add_to_values\n",
    "        num_embeddings = (\n",
    "            2 * max_relative_pos - 1\n",
    "            if unmasked\n",
    "            else max_relative_pos\n",
    "        )\n",
    "        embedding_size = (\n",
    "            [num_embeddings, embedding_dim, 1]\n",
    "            if heads_share_embeddings\n",
    "            else [num_heads, num_embeddings, embedding_dim, 1]\n",
    "        )\n",
    "        if add_to_values:\n",
    "            embedding_size[-1] = 2\n",
    "        initial_stddev = embedding_dim**(-0.5)\n",
    "        self.embeddings = nn.Parameter(torch.zeros(*embedding_size))\n",
    "        nn.init.normal_(self.embeddings, mean=0.0, std=initial_stddev)\n",
    "\n",
    "    def forward(self, query, saved_state=None):\n",
    "        \"\"\"\n",
    "        Computes relative positional embeddings to be added to keys (and optionally values),\n",
    "        multiplies the embeddings for keys with queries to create positional logits,\n",
    "        returns the positional logits, along with embeddings for values (optionally)\n",
    "        which could be added to values outside this module.\n",
    "        Args:\n",
    "            query (torch.Tensor): query tensor\n",
    "            saved_state (dict): saved state from previous time step\n",
    "        Shapes:\n",
    "            query: `(length, batch_size*num_heads, embed_dim)`\n",
    "        Returns:\n",
    "            tuple(torch.Tensor):\n",
    "                - positional logits\n",
    "                - relative positional embeddings to be added to values\n",
    "        \"\"\"\n",
    "        # During inference when previous states are cached\n",
    "        if saved_state is not None and \"prev_key\" in saved_state:\n",
    "            assert not self.unmasked, \"This should only be for decoder attention\"\n",
    "            length = saved_state[\"prev_key\"].shape[-2] + 1  # `length - 1` keys are cached,\n",
    "                                                            # `+ 1` for the current time step\n",
    "            decoder_step = True\n",
    "        else:\n",
    "            length = query.shape[0]\n",
    "            decoder_step = False\n",
    "\n",
    "        used_embeddings = self.get_embeddings_for_query(length)\n",
    "\n",
    "        values_embeddings = (\n",
    "            used_embeddings[..., 1]\n",
    "            if self.add_to_values\n",
    "            else None\n",
    "        )\n",
    "        positional_logits = self.calculate_positional_logits(query, used_embeddings[..., 0])\n",
    "        positional_logits = self.relative_to_absolute_indexing(positional_logits, decoder_step)\n",
    "        return (positional_logits, values_embeddings)\n",
    "\n",
    "    def get_embeddings_for_query(self, length):\n",
    "        \"\"\"\n",
    "        Extract the required embeddings. The maximum relative position between two time steps is\n",
    "        `length` for masked case or `2*length - 1` for the unmasked case. If `length` is greater than\n",
    "        `max_relative_pos`, we first pad the embeddings tensor with zero-embeddings, which represent\n",
    "        embeddings when relative position is greater than `max_relative_pos`. In case `length` is\n",
    "        less than `max_relative_pos`, we don't use the first `max_relative_pos - length embeddings`.\n",
    "        Args:\n",
    "            length (int): length of the query\n",
    "        Returns:\n",
    "            torch.Tensor: embeddings used by the query\n",
    "        \"\"\"\n",
    "        pad_length = max(length - self.max_relative_pos, 0)\n",
    "        start_pos = max(self.max_relative_pos - length, 0)\n",
    "        if self.unmasked:\n",
    "            with torch.no_grad():\n",
    "                padded_embeddings = nn.functional.pad(\n",
    "                    self.embeddings,\n",
    "                    (0, 0, 0, 0, pad_length, pad_length)\n",
    "                )\n",
    "            used_embeddings = padded_embeddings.narrow(-3, start_pos, 2*length - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                padded_embeddings = nn.functional.pad(\n",
    "                    self.embeddings,\n",
    "                    (0, 0, 0, 0, pad_length, 0)\n",
    "                )\n",
    "            used_embeddings = padded_embeddings.narrow(-3, start_pos, length)\n",
    "        return used_embeddings\n",
    "\n",
    "    def calculate_positional_logits(self, query, relative_embeddings):\n",
    "        \"\"\"\n",
    "        Multiplies query with the relative positional embeddings to create relative\n",
    "        positional logits\n",
    "        Args:\n",
    "            query (torch.Tensor): Input tensor representing queries\n",
    "            relative_embeddings (torch.Tensor): relative embeddings compatible with query\n",
    "        Shapes:\n",
    "            query: `(length, batch_size*num_heads, embed_dim)` if heads share embeddings\n",
    "                   else `(length, batch_size, num_heads, embed_dim)`\n",
    "            relative_embeddings: `(max_allowed_relative_positions, embed_dim)` if heads share embeddings\n",
    "                                 else `(num_heads, max_allowed_relative_positions, embed_dim)`\n",
    "                                 where `max_allowed_relative_positions` is `length` if masked\n",
    "                                 else `2*length - 1`\n",
    "        Returns:\n",
    "            torch.Tensor: relative positional logits\n",
    "        \"\"\"\n",
    "        if self.heads_share_embeddings:\n",
    "            positional_logits = torch.einsum(\"lbd,md->lbm\", query, relative_embeddings)\n",
    "        else:\n",
    "            query = query.view(query.shape[0], -1, self.num_heads, self.embedding_dim)\n",
    "            positional_logits = torch.einsum(\"lbhd,hmd->lbhm\", query, relative_embeddings)\n",
    "            positional_logits = positional_logits.contiguous().view(\n",
    "                positional_logits.shape[0], -1, positional_logits.shape[-1]\n",
    "            )\n",
    "        # mask out tokens out of range\n",
    "        length = query.size(0)\n",
    "        if length > self.max_relative_pos:\n",
    "            # there is some padding\n",
    "            pad_length = length - self.max_relative_pos\n",
    "            positional_logits[:,:,:pad_length] -= 1e8\n",
    "            if self.unmasked:\n",
    "                positional_logits[:,:,-pad_length:] -= 1e8\n",
    "        return positional_logits\n",
    "\n",
    "    def relative_to_absolute_indexing(self, x, decoder_step):\n",
    "        \"\"\"\n",
    "        Index tensor x (relative positional logits) in terms of absolute positions\n",
    "        rather than relative positions. Last dimension of x represents relative position\n",
    "        with respect to the first dimension, whereas returned tensor has both the first\n",
    "        and last dimension indexed with absolute positions.\n",
    "        Args:\n",
    "            x (torch.Tensor): positional logits indexed by relative positions\n",
    "            decoder_step (bool): is this is a single decoder step (during inference)\n",
    "        Shapes:\n",
    "            x: `(length, batch_size*num_heads, length)` for masked case or\n",
    "               `(length, batch_size*num_heads, 2*length - 1)` for unmasked\n",
    "        Returns:\n",
    "            torch.Tensor: positional logits represented using absolute positions\n",
    "        \"\"\"\n",
    "        length, bsz_heads, _ = x.shape\n",
    "\n",
    "        if decoder_step:\n",
    "            return x.contiguous().view(bsz_heads, 1, -1)\n",
    "\n",
    "        if self.unmasked:\n",
    "            x = nn.functional.pad(\n",
    "                x,\n",
    "                (0, 1)\n",
    "            )\n",
    "            x = x.transpose(0, 1)\n",
    "            x = x.contiguous().view(bsz_heads, length * 2 * length)\n",
    "            x = nn.functional.pad(\n",
    "                x,\n",
    "                (0, length - 1)\n",
    "            )\n",
    "            # Reshape and slice out the padded elements.\n",
    "            x = x.view(bsz_heads, length + 1, 2*length - 1)\n",
    "            return x[:, :length, length-1:]\n",
    "        else:\n",
    "            x = nn.functional.pad(\n",
    "                x,\n",
    "                (1, 0)\n",
    "            )\n",
    "            x = x.transpose(0, 1)\n",
    "            x = x.contiguous().view(bsz_heads, length+1, length)\n",
    "            return x[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    # Adapted from pytorch source\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, is_causal: bool = False) -> torch.Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.n_head = n_head\n",
    "    d_qkv = d_model // n_head\n",
    "    assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'\n",
    "    self.d_qkv = d_qkv\n",
    "\n",
    "    self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
    "    self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
    "    self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n",
    "    self.w_o = nn.Parameter(torch.Tensor(n_head, d_qkv, d_model))\n",
    "    nn.init.xavier_normal_(self.w_q)\n",
    "    nn.init.xavier_normal_(self.w_k)\n",
    "    nn.init.xavier_normal_(self.w_v)\n",
    "    nn.init.xavier_normal_(self.w_o)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    if relative_positional:\n",
    "        self.relative_positional = LearnedRelativePositionalEmbedding(relative_positional_distance, n_head, d_qkv, True)\n",
    "    else:\n",
    "        self.relative_positional = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Runs the multi-head self-attention layer.\n",
    "\n",
    "    Args:\n",
    "      x: the input to the layer, a tensor of shape [length, batch_size, d_model]\n",
    "    Returns:\n",
    "      A single tensor containing the output from this layer\n",
    "    \"\"\"\n",
    "\n",
    "    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)\n",
    "    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)\n",
    "    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)\n",
    "    logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)\n",
    "\n",
    "    if self.relative_positional is not None:\n",
    "        q_pos = q.permute(2,0,1,3) #bhqd->qbhd\n",
    "        l,b,h,d = q_pos.size()\n",
    "        position_logits, _ = self.relative_positional(q_pos.reshape(l,b*h,d))\n",
    "        # (bh)qk\n",
    "        logits = logits + position_logits.view(b,h,l,l)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs = self.dropout(probs)\n",
    "    o = torch.einsum('bhqk,bhka->bhqa', probs, v)\n",
    "    out = torch.einsum('bhta,haf->tbf', o, self.w_o)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 768\n",
    "num_layers = 6\n",
    "dropout = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transducer model that first applies the convolutional blocks to extract features which are fed into the transformer. 3.2.2  \n",
    "Note that in only x_raw (raw emg data) is used in the forward pass and not the manual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_ins, num_outs, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_ins, num_outs, 3, padding=1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(num_outs)\n",
    "        self.conv2 = nn.Conv1d(num_outs, num_outs, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_outs)\n",
    "\n",
    "        if stride != 1 or num_ins != num_outs:\n",
    "            self.residual_path = nn.Conv1d(num_ins, num_outs, 1, stride=stride)\n",
    "            self.res_norm = nn.BatchNorm1d(num_outs)\n",
    "        else:\n",
    "            self.residual_path = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_value = x\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "\n",
    "        if self.residual_path is not None:\n",
    "            res = self.res_norm(self.residual_path(input_value))\n",
    "        else:\n",
    "            res = input_value\n",
    "\n",
    "        return F.relu(x + res)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_outs, num_aux_outs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            ResBlock(8, model_size, 2),\n",
    "            ResBlock(model_size, model_size, 2),\n",
    "            ResBlock(model_size, model_size, 2),\n",
    "        )\n",
    "        self.w_raw_in = nn.Linear(model_size, model_size)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.w_out = nn.Linear(model_size, num_outs)\n",
    "\n",
    "        self.has_aux_out = num_aux_outs is not None\n",
    "        if self.has_aux_out:\n",
    "            self.w_aux = nn.Linear(model_size, num_aux_outs)\n",
    "\n",
    "    def forward(self, x_feat, x_raw, session_ids):\n",
    "        # x shape is (batch, time, electrode)\n",
    "\n",
    "        if self.training:\n",
    "            r = random.randrange(8)\n",
    "            if r > 0:\n",
    "                x_raw[:,:-r,:] = x_raw[:,r:,:] # shift left r\n",
    "                x_raw[:,-r:,:] = 0\n",
    "\n",
    "        x_raw = x_raw.transpose(1,2) # put channel before time for conv\n",
    "        x_raw = self.conv_blocks(x_raw)\n",
    "        x_raw = x_raw.transpose(1,2)\n",
    "        x_raw = self.w_raw_in(x_raw)\n",
    "\n",
    "        x = x_raw\n",
    "\n",
    "        x = x.transpose(0,1) # put time first\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0,1)\n",
    "\n",
    "        if self.has_aux_out:\n",
    "            return self.w_out(x), self.w_aux(x)\n",
    "        else:\n",
    "            return self.w_out(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decollate take a tensor and make it into a sequence of lists where each list is now of the appropriate length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decollate_tensor(tensor, lengths):\n",
    "    b, s, d = tensor.size()\n",
    "    tensor = tensor.view(b*s, d)\n",
    "    results = []\n",
    "    idx = 0\n",
    "    for length in lengths:\n",
    "        assert idx + length <= b * s\n",
    "        results.append(tensor[idx:idx+length])\n",
    "        idx += length\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_warp does the dynamic programming step of calculating the cummulative cost when going through the sequences. It takes the cost matrix as input, ie the euclidean distance between all points of the sequences. \n",
    "align_from_distance goes through the time warping and extracts the alignment, ie the indicies of ones sequence to align with the other. 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def time_warp(costs):\n",
    "    dtw = np.zeros_like(costs)\n",
    "    dtw[0,1:] = np.inf\n",
    "    dtw[1:,0] = np.inf\n",
    "    eps = 1e-4\n",
    "    for i in range(1,costs.shape[0]):\n",
    "        for j in range(1,costs.shape[1]):\n",
    "            dtw[i,j] = costs[i,j] + min(dtw[i-1,j],dtw[i,j-1],dtw[i-1,j-1])\n",
    "    return dtw\n",
    "\n",
    "def align_from_distances(distance_matrix, debug=False):\n",
    "    # for each position in spectrum 1, returns best match position in spectrum2\n",
    "    # using monotonic alignment\n",
    "    dtw = time_warp(distance_matrix)\n",
    "\n",
    "    i = distance_matrix.shape[0]-1\n",
    "    j = distance_matrix.shape[1]-1\n",
    "    results = [0] * distance_matrix.shape[0]\n",
    "    while i > 0 and j > 0:\n",
    "        results[i] = j\n",
    "        i, j = min([(i-1,j),(i,j-1),(i-1,j-1)], key=lambda x: dtw[x[0],x[1]])\n",
    "\n",
    "    if debug:\n",
    "        visual = np.zeros_like(dtw)\n",
    "        visual[range(len(results)),results] = 1\n",
    "        plt.matshow(visual)\n",
    "        plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtw_loss takes in audio feature prediction, phoneneme predictions and example which contains true labels for both audio features and phonemes. It aligns the predicted audio features with the true audio features. After cost matrix is calculated, the phoneme loss is added to it based on negative log probabilities.   \n",
    "For voiced examples, the pairwise distance is calculated directly as well as cross-entropy phoneme loss.  \n",
    "returns average loss in batch and accuracy of phoneme prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtw_loss(predictions, phoneme_predictions, example, phoneme_eval=False, phoneme_confusion=None):\n",
    "    device = predictions.device\n",
    "    phoneme_loss_weight = 0.5\n",
    "\n",
    "    predictions = decollate_tensor(predictions, example['lengths'])\n",
    "    phoneme_predictions = decollate_tensor(phoneme_predictions, example['lengths'])\n",
    "\n",
    "    audio_features = [t.to(device, non_blocking=True) for t in example['audio_features']]\n",
    "\n",
    "    phoneme_targets = example['phonemes']\n",
    "\n",
    "    losses = []\n",
    "    correct_phones = 0\n",
    "    total_length = 0\n",
    "    for pred, y, pred_phone, y_phone, silent in zip(predictions, audio_features, phoneme_predictions, phoneme_targets, example['silent']):\n",
    "        assert len(pred.size()) == 2 and len(y.size()) == 2\n",
    "        y_phone = y_phone.to(device)\n",
    "\n",
    "        if silent:\n",
    "            dists = torch.cdist(pred.unsqueeze(0), y.unsqueeze(0))\n",
    "            costs = dists.squeeze(0)\n",
    "\n",
    "            # pred_phone (seq1_len, 48), y_phone (seq2_len)\n",
    "            # phone_probs (seq1_len, seq2_len)\n",
    "            pred_phone = F.log_softmax(pred_phone, -1)\n",
    "            phone_lprobs = pred_phone[:,y_phone]\n",
    "\n",
    "            costs = costs + phoneme_loss_weight * -phone_lprobs\n",
    "\n",
    "            alignment = align_from_distances(costs.T.cpu().detach().numpy())\n",
    "\n",
    "            loss = costs[alignment,range(len(alignment))].sum()\n",
    "\n",
    "            if phoneme_eval:\n",
    "                alignment = align_from_distances(costs.T.cpu().detach().numpy())\n",
    "\n",
    "                pred_phone = pred_phone.argmax(-1)\n",
    "                correct_phones += (pred_phone[alignment] == y_phone).sum().item()\n",
    "\n",
    "                for p, t in zip(pred_phone[alignment].tolist(), y_phone.tolist()):\n",
    "                    phoneme_confusion[p, t] += 1\n",
    "        else:\n",
    "            assert y.size(0) == pred.size(0)\n",
    "\n",
    "            dists = F.pairwise_distance(y, pred)\n",
    "\n",
    "            assert len(pred_phone.size()) == 2 and len(y_phone.size()) == 1\n",
    "            phoneme_loss = F.cross_entropy(pred_phone, y_phone, reduction='sum')\n",
    "            loss = dists.sum() + phoneme_loss_weight * phoneme_loss\n",
    "\n",
    "            if phoneme_eval:\n",
    "                pred_phone = pred_phone.argmax(-1)\n",
    "                correct_phones += (pred_phone == y_phone).sum().item()\n",
    "\n",
    "                for p, t in zip(pred_phone.tolist(), y_phone.tolist()):\n",
    "                    phoneme_confusion[p, t] += 1\n",
    "\n",
    "        losses.append(loss)\n",
    "        total_length += y.size(0)\n",
    "\n",
    "    return sum(losses)/total_length, correct_phones/total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Vocoder: Converts audio features on audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        \n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "LRELU_SLOPE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock1(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs1:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class ResBlock2(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1])))\n",
    "        ])\n",
    "        self.convs.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs:\n",
    "            remove_weight_norm(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(Generator, self).__init__()\n",
    "        self.h = h\n",
    "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(h.upsample_rates)\n",
    "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
    "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = h.upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(h, ch, k, d))\n",
    "\n",
    "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
    "        self.ups.apply(init_weights)\n",
    "        self.conv_post.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        print('Removing weight norm...')\n",
    "        for l in self.ups:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.resblocks:\n",
    "            l.remove_weight_norm()\n",
    "        remove_weight_norm(self.conv_pre)\n",
    "        remove_weight_norm(self.conv_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./models/pretrained_models/hifigan_finetuned')\n",
    "hifigan_checkpoint = './models/pretrained_models/hifigan_finetuned/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocoder(object):\n",
    "    def __init__(self, device='cuda'):\n",
    "        #assert FLAGS.hifigan_checkpoint is not None\n",
    "        checkpoint_file = hifigan_checkpoint\n",
    "        #config_file = os.path.join(os.path.split(checkpoint_file)[0], 'config.json')\n",
    "        config_file = './models/pretrained_models/hifigan_finetuned/config.json'\n",
    "        with open(config_file) as f:\n",
    "            hparams = AttrDict(json.load(f))\n",
    "        self.generator = Generator(hparams).to(device)\n",
    "        self.generator.load_state_dict(torch.load(checkpoint_file)['generator'])\n",
    "        self.generator.eval()\n",
    "        self.generator.remove_weight_norm()\n",
    "\n",
    "    def __call__(self, mel_spectrogram):\n",
    "        '''\n",
    "            mel_spectrogram should be a tensor of shape (seq_len, 80)\n",
    "            returns 1d tensor of audio\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            mel_spectrogram = mel_spectrogram.T[np.newaxis,:,:]\n",
    "            audio = self.generator(mel_spectrogram)\n",
    "        return audio.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine_fixed_length takes a list of tensors and converts it into a single tensor with appropriate padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fixed_length(tensor_list, length):\n",
    "    total_length = sum(t.size(0) for t in tensor_list)\n",
    "    if total_length % length != 0:\n",
    "        pad_length = length - (total_length % length)\n",
    "        tensor_list = list(tensor_list) # copy\n",
    "        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))\n",
    "        total_length += pad_length\n",
    "    tensor = torch.cat(tensor_list, 0)\n",
    "    n = total_length // length\n",
    "    return tensor.view(n, length, *tensor.size()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function iterates over the test set, obtains the manual emg features(which are not used in the model), raw emg data and session id, and pass it into the model. The loss and phoneme loss are then calcualted based on the predicted and true values, using the dtw_loss.  \n",
    "It returns the mean loss, mean accuracy and phoneme confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testset, device):\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(testset, batch_size=32, collate_fn=testset.collate_raw)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    phoneme_confusion = np.zeros((len(phoneme_inventory),len(phoneme_inventory)))\n",
    "    seq_len = 200\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, 'Validation', disable=None):\n",
    "            X = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['emg']], seq_len)\n",
    "            X_raw = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['raw_emg']], seq_len*8)\n",
    "            sess = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['session_ids']], seq_len)\n",
    "\n",
    "            pred, phoneme_pred = model(X, X_raw, sess)\n",
    "\n",
    "            loss, phon_acc = dtw_loss(pred, phoneme_pred, batch, True, phoneme_confusion)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            accuracies.append(phon_acc)\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(losses), np.mean(accuracies), phoneme_confusion #TODO size-weight average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate function use the pretrained DeepSpeech model to calculate the word error rate between model's transcription and target transcription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(testset, audio_directory):\n",
    "    model = deepspeech.Model('deepspeech-0.7.0-models.pbmm')\n",
    "    model.enableExternalScorer('deepspeech-0.7.0-models.scorer')\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for i, datapoint in enumerate(tqdm(testset, 'Evaluate outputs', disable=None)):\n",
    "        audio, rate = sf.read(os.path.join(audio_directory,f'example_output_{i}.wav'))\n",
    "        if rate != 16000:\n",
    "            audio = librosa.resample(audio, orig_sr=rate, target_sr=16000)\n",
    "        assert model.sampleRate() == 16000, 'wrong sample rate'\n",
    "        audio_int16 = (audio*(2**15)).astype(np.int16)\n",
    "        text = model.stt(audio_int16)\n",
    "        predictions.append(text)\n",
    "        target_text = unidecode(datapoint['text'])\n",
    "        targets.append(target_text)\n",
    "    transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "    targets = transformation(targets)\n",
    "    predictions = transformation(predictions)\n",
    "    logging.info(f'targets: {targets}')\n",
    "    logging.info(f'predictions: {predictions}')\n",
    "    logging.info(f'wer: {jiwer.wer(targets, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SizeAwareSampler creates batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeAwareSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, emg_dataset, max_len):\n",
    "        self.dataset = emg_dataset\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        random.shuffle(indices)\n",
    "        batch = []\n",
    "        batch_length = 0\n",
    "        for idx in indices:\n",
    "            directory_info, file_idx = self.dataset.example_indices[idx]\n",
    "            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:\n",
    "                info = json.load(f)\n",
    "            if not np.any([l in string.ascii_letters for l in info['text']]):\n",
    "                continue\n",
    "            length = sum([emg_len for emg_len, _, _ in info['chunks']])\n",
    "            if length > self.max_len:\n",
    "                logging.warning(f'Warning: example {idx} cannot fit within desired batch length')\n",
    "            if length + batch_length > self.max_len:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                batch_length = 0\n",
    "            batch.append(idx)\n",
    "            batch_length += length\n",
    "        # dropping last incomplete batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_output use the vocoder to generate the output audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(model, datapoint, filename, device, audio_normalizer, vocoder):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sess = datapoint['session_ids'].to(device=device).unsqueeze(0)\n",
    "        X = datapoint['emg'].to(dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        X_raw = datapoint['raw_emg'].to(dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        pred, _ = model(X, X_raw, sess)\n",
    "        y = pred.squeeze(0)\n",
    "\n",
    "        y = audio_normalizer.inverse(y.cpu()).to(device)\n",
    "\n",
    "        audio = vocoder(y).cpu().numpy()\n",
    "\n",
    "    sf.write(filename, audio, 22050)\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop creates the dataloader, loading batches according ith sizeawaresampler, initilize the transducer model and vocoder. Then looping thorugh the batches, obtaining the manual emg features, raw emg data and session id for each batch to pass into the model. The dtw_loss is then used before backpropagation. The validation set is used in the test function after each epoch and the output audio is saved. After all epochs, the WER is calculated on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_model(trainset, devset, device, n_epochs=80, data_size_fraction=1.0, save_sound_outputs=True, \n",
    "                learning_rate=1e-3, learning_rate_warmup=500, weight_decay=1e-7, batch_sequence_length=200, batch_raw_length=1600, patience=5):\n",
    "    \n",
    "    # Subset the training data if necessary\n",
    "    if data_size_fraction >= 1:\n",
    "        training_subset = trainset\n",
    "    else:\n",
    "        training_subset = trainset.subset(data_size_fraction)\n",
    "        \n",
    "    # Define the dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(training_subset, pin_memory=(device=='cuda'), collate_fn=devset.collate_raw, \n",
    "                                             num_workers=0, batch_sampler=SizeAwareSampler(training_subset, 256000))\n",
    "    \n",
    "    # Initialize the model\n",
    "    n_phones = len(phoneme_inventory)\n",
    "    model = Model(devset.num_features, devset.num_speech_features, n_phones).to(device)\n",
    "    \n",
    "    # Optionally load model weights\n",
    "    if save_sound_outputs:\n",
    "        vocoder = Vocoder()\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', 0.5, patience=patience)\n",
    "\n",
    "    def set_lr(new_lr):\n",
    "        for param_group in optim.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "    def schedule_lr(iteration):\n",
    "        if iteration <= learning_rate_warmup:\n",
    "            set_lr(iteration * learning_rate / learning_rate_warmup)\n",
    "            \n",
    "    seq_len = 200\n",
    "    batch_idx = 0\n",
    "    n_epochs = 2\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        losses = []\n",
    "        for batch in tqdm(dataloader, 'Train step', disable=None):\n",
    "            optim.zero_grad()\n",
    "            schedule_lr(batch_idx)\n",
    "            \n",
    "            X = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['emg']], seq_len)\n",
    "            X_raw = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['raw_emg']], seq_len*8)\n",
    "            sess = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['session_ids']], seq_len)\n",
    "\n",
    "            pred, phoneme_pred = model(X, X_raw, sess)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss, _ = dtw_loss(pred, phoneme_pred, batch)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        # Calculate mean training loss\n",
    "        train_loss = np.mean(losses)\n",
    "\n",
    "        # Validation step\n",
    "        val, phoneme_acc, _ = test(model, devset, device)\n",
    "        lr_sched.step(val)\n",
    "        logging.info(f'finished epoch {epoch_idx+1} - validation loss: {val:.4f} training loss: {train_loss:.4f} phoneme accuracy: {phoneme_acc*100:.2f}')\n",
    "        \n",
    "        # Save the model\n",
    "        #torch.save(model.state_dict(), os.path.join(output_directory, 'model.pt'))\n",
    "\n",
    "        # Save example outputs if required\n",
    "        if save_sound_outputs:\n",
    "            save_output(model, devset[0], os.path.join(output_directory, f'epoch_{epoch_idx}_output.wav'), device, devset.mfcc_norm, vocoder)\n",
    "        \n",
    "        print(epoch_idx)\n",
    "\n",
    "    # Save additional outputs after training if necessary\n",
    "    if save_sound_outputs:\n",
    "        for i, datapoint in enumerate(devset):\n",
    "            save_output(model, datapoint, os.path.join(output_directory, f'example_output_{i}.wav'), device, devset.mfcc_norm, vocoder)\n",
    "\n",
    "        evaluate(devset, output_directory)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for training transducer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = EMGDataset(dev=False,test=False)\n",
    "devset = EMGDataset(dev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1366c7236dd40bd8d93c561af45da12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23137bf0e37549709e8c7949f3cccab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1acad70e266460ba2214be09ffc0482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfff3e7bf7674ba6a2312ec2dac2f556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow: v2.3.0-6-g23ad988\n",
      "DeepSpeech: v0.9.3-0-gf2e9c85\n",
      "2024-10-28 06:49:06.212710: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85de34e380474a63901705f8a28ae8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluate outputs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "use_hifigan = True\n",
    "model = train_model(trainset, devset, device, save_sound_outputs=(use_hifigan is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silent_speech2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
