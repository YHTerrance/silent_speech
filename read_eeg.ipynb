{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib import *\n",
    "from read_eeg import create_datasets, collate_fn\n",
    "\n",
    "base_dir = Path(\"/ocean/projects/cis240129p/shared/data/eeg_alice\")\n",
    "subjects_used = [\"S04\"]  # exclude 'S05' - less channels # , \"S13\", \"S19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    }
   ],
   "source": [
    "ds = BrennanSeqDataset(\n",
    "    root_dir=base_dir,\n",
    "    phoneme_dir=base_dir / \"phonemes\",\n",
    "    idx=\"S04\",\n",
    "    seq_len=5,\n",
    "    phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice was beginning to get\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'audio_feats', 'audio_raw', 'eeg_raw', 'eeg_feats', 'phonemes'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = ds[0]\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 128), (2600, 60),(795, 300), (624,)\n",
      "0 Alice was beginning to get\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "1 very tired of sitting by\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "2 her sister on the bank\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "3 and of having nothing to\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "4 do once or twice she\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "5 peeped into the book her\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "6 sister was reading but it\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "7 had no pictures or conversations\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "8 in it and what \u001as\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "9 the use of a book\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "10 thought Alice without pictures or\n",
      "(650, 128), (2600, 60),(795, 300), (650,)\n",
      "11 conversation So she was considering\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(ds):\n",
    "    # print data shape\n",
    "    print(\n",
    "        f\"{data['audio_feats'].shape}, {data['eeg_raw'].shape},{data['eeg_feats'].shape}, {data['phonemes'].shape}\"\n",
    "    )\n",
    "    print(i, data[\"labels\"])\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item[\"audio_feats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice\n",
      "dict_keys(['label', 'audio_feats', 'audio_raw', 'eeg_raw', 'eeg_feats', 'phonemes'])\n",
      "(104, 128) (159, 300) 104\n"
     ]
    }
   ],
   "source": [
    "item2 = ds_brennan[0]\n",
    "print(item2.keys())\n",
    "print(item2[\"audio_feats\"].shape, item2[\"eeg_feats\"].shape, len(item2[\"phonemes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104, 128), (159, 300), 104)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"audio_feats\"][0].shape, item[\"eeg_feats\"][0].shape, len(item[\"phonemes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    }
   ],
   "source": [
    "ds_brennan = BrennanDataset(\n",
    "    root_dir=base_dir,\n",
    "    phoneme_dir=base_dir / \"phonemes\",\n",
    "    idx=\"S04\",\n",
    "    phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice\n",
      "dict_keys(['label', 'audio_feats', 'audio_raw', 'eeg_raw', 'eeg_feats', 'phonemes'])\n",
      "(104, 128) (159, 300) 104\n"
     ]
    }
   ],
   "source": [
    "item2 = ds_brennan[0]\n",
    "print(item2.keys())\n",
    "print(item2[\"audio_feats\"].shape, item2[\"eeg_feats\"].shape, len(item2[\"phonemes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1703, Test dataset length: 426\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(subjects_used, base_dir)\n",
    "\n",
    "print(\n",
    "    f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     A custom collate function that handles different types of data in a batch.\n",
    "#     It dynamically creates batches by converting arrays or lists to tensors and\n",
    "#     applies padding to variable-length sequences.\n",
    "#     \"\"\"\n",
    "#     batch_dict = {}\n",
    "#     for key in batch[0].keys():\n",
    "#         batch_items = [item[key] for item in batch]\n",
    "#         if isinstance(batch_items[0], np.ndarray) or isinstance(\n",
    "#             batch_items[0], torch.Tensor\n",
    "#         ):\n",
    "#             if isinstance(batch_items[0], np.ndarray):\n",
    "#                 batch_items = [torch.tensor(b) for b in batch_items]\n",
    "#             if len(batch_items[0].shape) > 0:\n",
    "#                 batch_dict[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "#                     batch_items, batch_first=True  # pad with zeros\n",
    "#                 )\n",
    "#             else:\n",
    "#                 batch_dict[key] = torch.stack(batch_items)\n",
    "#         else:\n",
    "#             batch_dict[key] = batch_items\n",
    "\n",
    "#     return batch_dict\n",
    "\n",
    "\n",
    "train_dataloder = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label <class 'str'>\n",
      "audio_feats (104, 128) <class 'numpy.ndarray'>\n",
      "audio_raw (16735,) <class 'numpy.ndarray'>\n",
      "eeg_raw (520, 62) <class 'numpy.ndarray'>\n",
      "eeg_feats (159, 310) <class 'numpy.ndarray'>\n",
      "phonemes (104,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k, v in item.items():\n",
    "    try:\n",
    "        print(k, v.shape, type(v))\n",
    "    except:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"eeg_raw\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "1\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "2\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "3\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "4\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "i = 0\n",
    "for batch in train_dataloder:\n",
    "    print(i)\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            print(k, v.shape, type(v))\n",
    "        except:\n",
    "            print(k, type(v))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
