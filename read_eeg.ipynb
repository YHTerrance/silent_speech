{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/ocean/projects/cis240129p/soederha/silent_speech\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib import BrennanDataset\n",
    "\n",
    "base_dir = Path(\"/ocean/projects/cis240129p/shared/data/eeg_alice\")\n",
    "subjects_used = [\"S04\", \"S13\", \"S19\"]  # exclude 'S05' - less channels\n",
    "\n",
    "# ds = BrennanDataset(\n",
    "#     root_dir=base_dir,\n",
    "#     phoneme_dir=base_dir / \"phonemes\",\n",
    "#     idx=\"S01\",\n",
    "#     phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S13.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368274  =      0.000 ...   736.548 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S19.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 373374  =      0.000 ...   746.748 secs...\n",
      "Train dataset length: 5109, Test dataset length: 1278\n"
     ]
    }
   ],
   "source": [
    "def create_datasets(subjects, base_dir):\n",
    "    train_datasets = []\n",
    "    test_datasets = []\n",
    "    for subject in subjects:\n",
    "        dataset = BrennanDataset(\n",
    "            root_dir=base_dir,\n",
    "            phoneme_dir=base_dir / \"phonemes\",\n",
    "            idx=subject,\n",
    "            phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "        )\n",
    "        num_data_points = len(dataset)\n",
    "\n",
    "        # Split indices into train and test sets\n",
    "        split_index = int(num_data_points * 0.8)\n",
    "        train_indices = list(range(split_index))\n",
    "        test_indices = list(range(split_index, num_data_points))\n",
    "\n",
    "        # Create Subset datasets using indices\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "        train_datasets.append(train_dataset)\n",
    "        test_datasets.append(test_dataset)\n",
    "    return train_datasets, test_datasets\n",
    "\n",
    "\n",
    "train_ds, test_ds = create_datasets(subjects_used, base_dir)\n",
    "train_dataset = ConcatDataset(train_ds)\n",
    "test_dataset = ConcatDataset(test_ds)\n",
    "print(\n",
    "    f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function that handles different types of data in a batch.\n",
    "    It dynamically creates batches by converting arrays or lists to tensors and\n",
    "    applies padding to variable-length sequences.\n",
    "    \"\"\"\n",
    "    batch_dict = {}\n",
    "    for key in batch[0].keys():\n",
    "        batch_items = [item[key] for item in batch]\n",
    "        if isinstance(batch_items[0], np.ndarray) or isinstance(\n",
    "            batch_items[0], torch.Tensor\n",
    "        ):\n",
    "            if isinstance(batch_items[0], np.ndarray):\n",
    "                batch_items = [torch.tensor(b) for b in batch_items]\n",
    "            if len(batch_items[0].shape) > 0:\n",
    "                batch_dict[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "                    batch_items, batch_first=True  # pad with zeros\n",
    "                )\n",
    "            else:\n",
    "                batch_dict[key] = torch.stack(batch_items)\n",
    "        else:\n",
    "            batch_dict[key] = batch_items\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "train_dataloder = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'wind']\n",
      "['fallen', 'a']\n",
      "['the', 'time']\n",
      "['to', 'it']\n",
      "['bat', 'getting']\n",
      "['center', 'It']\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "#print(item['eeg_feats'].shape)\n",
    "'''for k, v in item.items():\n",
    "    print(k)\n",
    "    try:\n",
    "        print(k, v.shape, type(v))\n",
    "    except:\n",
    "        print(k, type(v))'''\n",
    "        \n",
    "i=0\n",
    "for batch in train_dataloder:\n",
    "    print(batch['label'])\n",
    "    if i > 4:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label\n",
      "label <class 'list'>\n",
      "audio_feats\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "tensor([[[ -6.7778,  -6.8072,  -6.8244,  ..., -10.9782, -11.0637, -11.4997],\n",
      "         [ -7.1028,  -7.7666, -11.2400,  ..., -10.1732, -10.6435, -11.4568],\n",
      "         [ -9.6935,  -6.3227,  -5.6189,  ...,  -9.2405, -10.0268, -11.3830],\n",
      "         ...,\n",
      "         [ -7.1799,  -7.4217,  -7.7385,  ..., -11.3257, -11.4271, -11.5099],\n",
      "         [ -8.7858,  -7.6728,  -7.1338,  ..., -10.9809, -11.4416, -11.5081],\n",
      "         [ -6.7073,  -6.2596,  -5.9299,  ..., -11.2956, -11.3942, -11.4918]],\n",
      "\n",
      "        [[ -6.3884,  -7.0069,  -9.1198,  ..., -11.4579, -11.4999, -11.5121],\n",
      "         [ -6.1568,  -6.6722,  -7.8415,  ..., -11.4823, -11.4937, -11.5111],\n",
      "         [ -6.6638,  -6.6713,  -6.6651,  ..., -11.4457, -11.4955, -11.5119],\n",
      "         ...,\n",
      "         [ -9.8596,  -9.6112,  -9.3950,  ..., -11.4810, -11.5087, -11.5129],\n",
      "         [-10.8165, -10.8182, -10.8129,  ..., -11.5027, -11.5126, -11.5129],\n",
      "         [ -7.8479,  -7.9761,  -8.1142,  ..., -11.4681, -11.5122, -11.5129]]])\n",
      "audio_raw\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "tensor([[-0.1424, -0.1407, -0.1304,  ...,  0.1726,  0.1718,  0.1703],\n",
      "        [-0.0040,  0.0046,  0.0123,  ...,  0.0973,  0.1317,  0.1500]],\n",
      "       dtype=torch.float64)\n",
      "eeg_raw\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "tensor([[[-3.2049e-07, -1.3999e-06, -1.1850e-06,  ..., -8.3031e-07,\n",
      "           8.9106e-04,  1.7003e-02],\n",
      "         [-6.6090e-06, -7.1925e-06, -6.8911e-06,  ..., -9.3120e-06,\n",
      "          -1.3400e-03,  1.4519e-02],\n",
      "         [-2.0158e-06, -3.7200e-06, -1.1533e-06,  ..., -7.5692e-06,\n",
      "           1.4918e-03, -1.5402e-02],\n",
      "         ...,\n",
      "         [-4.8031e-06, -4.2622e-06, -5.1891e-06,  ..., -5.7719e-06,\n",
      "          -3.7021e-04, -4.3739e-02],\n",
      "         [-4.8799e-06, -3.4644e-06, -4.3279e-06,  ..., -3.6549e-06,\n",
      "           3.0991e-04, -4.2922e-02],\n",
      "         [-1.0920e-06, -8.3090e-07, -7.1299e-07,  ..., -1.0684e-07,\n",
      "          -2.6963e-05, -2.0199e-02]],\n",
      "\n",
      "        [[ 1.8065e-06,  2.8233e-06,  1.9550e-06,  ...,  4.0025e-06,\n",
      "          -2.5781e-04, -1.0304e-02],\n",
      "         [ 3.9021e-06,  5.6229e-06,  1.8894e-06,  ...,  8.6373e-06,\n",
      "          -7.2404e-04, -1.5217e-02],\n",
      "         [ 4.1486e-06,  4.7418e-06,  1.4817e-06,  ...,  6.5313e-06,\n",
      "           7.3116e-04, -5.5376e-03],\n",
      "         ...,\n",
      "         [ 8.7953e-06,  8.2126e-06,  6.7662e-06,  ..., -1.2383e-05,\n",
      "           5.5316e-04,  6.1914e-02],\n",
      "         [ 7.4847e-06,  7.0417e-06,  6.5744e-06,  ..., -9.3177e-06,\n",
      "           1.7841e-03,  9.4681e-03],\n",
      "         [ 3.8907e-06,  2.1198e-06,  2.3751e-06,  ..., -8.2437e-06,\n",
      "           2.5383e-04,  1.7621e-02]]], dtype=torch.float64)\n",
      "eeg_feats\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "tensor([[[-4.2127e-06,  4.6487e-06,  2.6696e-06,  ...,  2.6486e-02,\n",
      "           4.3750e-01,  2.4424e-02],\n",
      "         [-6.1104e-06,  6.4873e-06,  2.2314e-06,  ...,  2.9753e-02,\n",
      "           4.3750e-01,  2.7364e-02],\n",
      "         [-8.4179e-06,  8.9086e-06,  2.9269e-06,  ...,  3.4228e-02,\n",
      "           4.3750e-01,  3.0627e-02],\n",
      "         ...,\n",
      "         [-5.7178e-06,  7.1660e-06,  4.0511e-06,  ...,  3.1070e-03,\n",
      "           3.7500e-01,  2.2039e-03],\n",
      "         [-2.6191e-06,  5.1304e-06,  3.1278e-06,  ...,  4.9614e-03,\n",
      "           2.5000e-01,  3.5168e-03],\n",
      "         [-2.0990e-07,  2.6212e-06,  3.4608e-06,  ...,  7.8375e-03,\n",
      "           3.1250e-01,  5.8243e-03]],\n",
      "\n",
      "        [[ 9.2849e-06,  9.9590e-06,  1.8768e-06,  ...,  5.8513e-03,\n",
      "           5.6250e-01,  5.0425e-03],\n",
      "         [ 1.1084e-05,  1.1214e-05,  1.9201e-06,  ...,  5.1073e-03,\n",
      "           6.2500e-01,  4.3751e-03],\n",
      "         [ 1.1584e-05,  1.1617e-05,  2.4520e-06,  ...,  3.9365e-03,\n",
      "           5.6250e-01,  3.4189e-03],\n",
      "         ...,\n",
      "         [-1.4417e-06,  3.8495e-06,  2.2892e-06,  ...,  1.8113e-02,\n",
      "           4.3750e-01,  1.4402e-02],\n",
      "         [ 1.6313e-06,  4.7959e-06,  2.4962e-06,  ...,  2.3334e-02,\n",
      "           5.0000e-01,  1.7509e-02],\n",
      "         [ 4.2996e-06,  5.5159e-06,  2.7611e-06,  ...,  2.7015e-02,\n",
      "           5.0000e-01,  2.1304e-02]]])\n",
      "phonemes\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "tensor([[15,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 13, 13, 13, 13,\n",
      "         13, 13, 22, 22, 22, 22, 22, 26, 26, 26,  0,  0,  2,  2,  2,  2,  2, 22,\n",
      "         22, 22, 22, 22, 22,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "          2, 14, 14, 14, 14, 14, 14, 14, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13, 13, 13, 13, 22, 22, 22,\n",
      "         22, 22, 22, 22],\n",
      "        [38, 38, 38, 38, 38, 38, 38, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  0,  0, 14, 14, 14, 14, 14, 14,\n",
      "         14, 14, 14, 14, 23, 23, 23, 23, 23,  0,  0,  6,  6,  6,  6,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5, 35, 35, 35,  0,  0, 21, 21, 21, 13, 13, 13, 13, 13,\n",
      "         13,  0,  0, 35, 35, 35, 35, 35, 35, 35, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 39, 39, 39, 39, 39, 39, 13, 13, 13, 13, 13, 14, 14, 14,\n",
      "          0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 15, 15, 15, 15, 15, 15,\n",
      "         15, 15, 15, 15]])\n",
      "1\n",
      "label\n",
      "label <class 'list'>\n",
      "audio_feats\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "tensor([[[ -9.0569,  -9.1106,  -9.1564,  ..., -10.6267, -11.3465, -11.5105],\n",
      "         [ -8.8788,  -8.8919,  -8.8927,  ..., -10.4489, -11.3869, -11.5114],\n",
      "         [ -9.6017,  -9.2194,  -8.9236,  ..., -10.3014, -11.4044, -11.5110],\n",
      "         ...,\n",
      "         [ -5.0430,  -5.3748,  -5.8796,  ..., -11.3271, -11.4035, -11.5110],\n",
      "         [ -9.7796,  -9.3071,  -8.9667,  ..., -11.4003, -11.4537, -11.5066],\n",
      "         [ -5.7069,  -6.2504,  -7.5943,  ..., -11.3508, -11.4525, -11.5068]],\n",
      "\n",
      "        [[ -9.9209, -10.3044, -10.9541,  ...,  -9.6066, -11.2920, -11.5036],\n",
      "         [ -9.4430,  -9.0163,  -8.6976,  ...,  -9.2155, -11.4430, -11.5094],\n",
      "         [ -9.9811, -10.1370, -10.3178,  ..., -10.4608, -11.3758, -11.5036],\n",
      "         ...,\n",
      "         [ -8.0846,  -7.8336,  -7.6142,  ..., -11.1888, -11.4861, -11.5116],\n",
      "         [ -8.4970,  -9.0750, -10.7008,  ..., -11.2490, -11.4801, -11.5124],\n",
      "         [ -9.8576,  -8.8910,  -8.3846,  ..., -10.8910, -11.4611, -11.5125]]])\n",
      "audio_raw\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "tensor([[ 0.0370, -0.0309, -0.1043,  ...,  0.1616,  0.1784,  0.1916],\n",
      "        [-0.1618, -0.1739, -0.1615,  ...,  0.0529,  0.0548,  0.0571]],\n",
      "       dtype=torch.float64)\n",
      "eeg_raw\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "tensor([[[ 4.5227e-07,  2.9986e-07,  5.1384e-07,  ..., -6.1000e-06,\n",
      "          -7.2433e-04,  3.3903e-02],\n",
      "         [ 9.4753e-07,  1.8877e-06,  1.2606e-07,  ..., -9.0698e-06,\n",
      "          -3.2223e-03,  2.0473e-02],\n",
      "         [ 2.8741e-07,  1.0602e-06, -7.4759e-07,  ..., -1.4110e-05,\n",
      "          -3.5491e-03,  1.3847e-04],\n",
      "         ...,\n",
      "         [-5.9124e-06, -4.0517e-06, -2.0555e-07,  ..., -1.1744e-05,\n",
      "          -1.7500e-03, -2.9971e-02],\n",
      "         [-5.9734e-06, -4.0995e-06, -1.6439e-06,  ..., -2.2368e-05,\n",
      "          -2.4504e-04, -3.0598e-03],\n",
      "         [-4.8106e-06, -4.1434e-06, -2.9000e-06,  ..., -1.9094e-05,\n",
      "          -1.1346e-03, -3.7340e-02]],\n",
      "\n",
      "        [[-1.1348e-05, -1.1851e-05, -1.0303e-05,  ..., -2.1805e-05,\n",
      "           1.0823e-02, -1.0492e-02],\n",
      "         [-1.1126e-05, -1.3261e-05, -1.0668e-05,  ..., -1.7208e-05,\n",
      "           9.9114e-03, -9.3628e-03],\n",
      "         [-3.1994e-06, -3.2446e-06, -1.8252e-06,  ..., -7.5676e-06,\n",
      "           1.1240e-02, -8.4374e-03],\n",
      "         ...,\n",
      "         [ 3.1087e-06, -1.8768e-07, -6.1412e-07,  ..., -9.7861e-06,\n",
      "           1.4674e-03, -5.9269e-02],\n",
      "         [ 1.5981e-06, -2.3795e-06, -3.5357e-06,  ..., -6.7962e-06,\n",
      "           3.6558e-04, -6.3851e-02],\n",
      "         [-5.0061e-07, -2.7760e-06, -4.5958e-06,  ..., -4.5849e-06,\n",
      "           1.7394e-04, -3.5031e-02]]], dtype=torch.float64)\n",
      "eeg_feats\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "tensor([[[ 2.8486e-06,  3.8525e-06,  1.5383e-06,  ...,  6.3470e-02,\n",
      "           4.3750e-01,  4.6325e-02],\n",
      "         [ 4.9083e-06,  5.6457e-06,  1.6569e-06,  ...,  7.6588e-02,\n",
      "           5.6250e-01,  5.8783e-02],\n",
      "         [ 6.7852e-06,  7.0762e-06,  1.4605e-06,  ...,  7.8853e-02,\n",
      "           7.5000e-01,  6.3890e-02],\n",
      "         ...,\n",
      "         [-9.4470e-06,  9.8528e-06,  2.1085e-06,  ...,  6.9478e-02,\n",
      "           6.2500e-01,  6.0039e-02],\n",
      "         [-1.0893e-05,  1.1047e-05,  2.9840e-06,  ...,  5.6268e-02,\n",
      "           6.2500e-01,  4.7704e-02],\n",
      "         [-1.0595e-05,  1.0858e-05,  3.1607e-06,  ...,  4.3662e-02,\n",
      "           6.8750e-01,  3.7068e-02]],\n",
      "\n",
      "        [[ 3.4928e-06,  7.7998e-06,  4.7585e-06,  ...,  3.1407e-03,\n",
      "           3.1250e-01,  2.5543e-03],\n",
      "         [ 9.7818e-06,  1.3489e-05,  2.9046e-06,  ...,  2.8348e-03,\n",
      "           2.5000e-01,  2.3907e-03],\n",
      "         [ 1.6619e-05,  1.8576e-05,  2.8904e-06,  ...,  3.2343e-03,\n",
      "           2.5000e-01,  2.8786e-03],\n",
      "         ...,\n",
      "         [-5.5179e-06,  5.7717e-06,  1.6283e-06,  ...,  6.1929e-02,\n",
      "           5.6250e-01,  5.4634e-02],\n",
      "         [-3.8252e-06,  4.5471e-06,  1.8364e-06,  ...,  6.1611e-02,\n",
      "           6.2500e-01,  5.5584e-02],\n",
      "         [-1.9898e-06,  3.1062e-06,  1.9072e-06,  ...,  6.3067e-02,\n",
      "           5.6250e-01,  5.7223e-02]]])\n",
      "phonemes\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "tensor([[12, 12, 12, 12, 12, 36, 36, 36, 36, 36, 36, 36, 36, 13, 13, 13, 13, 13,\n",
      "         13, 13,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0, 39, 39, 39, 39, 39,  2,  2,  2,  2,  2, 22, 22, 22,  0,  0,\n",
      "          0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 36, 36, 36, 36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 38, 38,\n",
      "         38, 38, 38, 38, 38, 15, 15, 15, 15, 15, 15, 15,  0,  0, 37, 37, 37, 37,\n",
      "         37, 37, 37, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "         33, 22, 22, 22],\n",
      "        [ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "          5,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "         30, 30, 30, 30, 30,  0,  0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 18, 18,\n",
      "         18, 18, 18, 18, 18, 18, 22, 22, 22, 22, 22, 22,  5,  5,  5,  5,  5,  5,\n",
      "          5,  5,  5,  0, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 14,\n",
      "          0,  0,  0, 13, 13, 13, 13, 13, 13, 13, 24, 24, 24, 24, 24,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "2\n",
      "label\n",
      "label <class 'list'>\n",
      "audio_feats\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "tensor([[[ -8.7192,  -7.8402,  -7.3555,  ..., -11.2995, -11.4127, -11.4904],\n",
      "         [ -7.6600,  -7.4265,  -7.2185,  ..., -11.4016, -11.3883, -11.5088],\n",
      "         [ -6.8095,  -7.3327,  -8.5474,  ..., -11.4299, -11.4544, -11.5018],\n",
      "         ...,\n",
      "         [ -6.4394,  -7.0137,  -8.6011,  ..., -10.9488, -11.2794, -11.5050],\n",
      "         [ -5.6433,  -6.2301,  -7.9382,  ..., -10.0341, -11.2512, -11.5003],\n",
      "         [ -6.2036,  -6.4140,  -6.6757,  ...,  -9.9136, -11.0668, -11.5066]],\n",
      "\n",
      "        [[ -9.5606,  -8.2062,  -7.6254,  ..., -11.4675, -11.5046, -11.5126],\n",
      "         [ -9.1440,  -8.2089,  -7.7098,  ..., -11.4859, -11.5067, -11.5127],\n",
      "         [ -9.5531,  -9.0371,  -8.6771,  ..., -11.4966, -11.5078, -11.5127],\n",
      "         ...,\n",
      "         [-11.4702, -11.4827, -11.4954,  ..., -11.5119, -11.5126, -11.5129],\n",
      "         [-11.2430, -11.2230, -11.1990,  ..., -11.5122, -11.5128, -11.5129],\n",
      "         [-10.5254, -10.3767, -10.2337,  ..., -11.5109, -11.5124, -11.5129]]])\n",
      "audio_raw\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "tensor([[-0.0672, -0.0475, -0.0261,  ...,  0.1485,  0.1600,  0.1709],\n",
      "        [-0.0103, -0.0121, -0.0138,  ..., -0.0006,  0.0003,  0.0013]],\n",
      "       dtype=torch.float64)\n",
      "eeg_raw\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "tensor([[[ 4.2818e-06,  3.2522e-06,  1.7485e-06,  ...,  7.7727e-06,\n",
      "           1.7172e-03, -4.0008e-02],\n",
      "         [ 1.0071e-05,  1.0544e-05,  6.1572e-06,  ...,  2.0634e-05,\n",
      "           1.0561e-02, -4.2786e-02],\n",
      "         [ 2.1963e-05,  1.9882e-05,  1.4565e-05,  ...,  3.3510e-05,\n",
      "           8.0230e-03, -4.3980e-02],\n",
      "         ...,\n",
      "         [ 1.2238e-05,  1.1312e-05,  1.2460e-05,  ...,  8.7171e-06,\n",
      "           1.8839e-03,  3.7812e-02],\n",
      "         [ 8.8614e-06,  9.1994e-06,  1.1514e-05,  ...,  1.2453e-05,\n",
      "           3.4554e-03,  4.2123e-02],\n",
      "         [ 1.0157e-05,  9.4187e-06,  1.0456e-05,  ...,  1.1561e-05,\n",
      "           3.1512e-03,  1.8552e-02]],\n",
      "\n",
      "        [[ 4.6298e-06,  4.9055e-06,  3.9778e-06,  ..., -2.0968e-06,\n",
      "          -3.3597e-04,  3.6590e-03],\n",
      "         [ 3.0199e-06,  1.2652e-06,  4.6575e-06,  ..., -9.8500e-06,\n",
      "           5.7994e-04,  5.9882e-02],\n",
      "         [ 4.3533e-06,  3.5448e-06,  6.1115e-06,  ..., -3.9759e-06,\n",
      "           8.3833e-04,  2.4885e-01],\n",
      "         ...,\n",
      "         [-2.8552e-06, -1.8410e-06,  1.0360e-07,  ..., -6.7988e-06,\n",
      "          -1.4631e-04,  9.8117e-02],\n",
      "         [-2.6916e-06, -3.2298e-06, -1.9282e-06,  ..., -6.9455e-06,\n",
      "           1.7594e-03,  6.5562e-02],\n",
      "         [-1.4640e-06, -2.0585e-06, -1.6089e-06,  ..., -4.3381e-06,\n",
      "           1.7760e-03,  3.7392e-02]]], dtype=torch.float64)\n",
      "eeg_feats\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "tensor([[[ 1.4711e-05,  1.5101e-05,  7.4131e-06,  ...,  2.0715e-02,\n",
      "           2.5000e-01,  1.8792e-02],\n",
      "         [ 1.5039e-05,  1.5285e-05,  8.2301e-06,  ...,  2.2415e-02,\n",
      "           3.1250e-01,  1.8690e-02],\n",
      "         [ 1.3430e-05,  1.3686e-05,  8.2404e-06,  ...,  2.4771e-02,\n",
      "           3.7500e-01,  2.1129e-02],\n",
      "         ...,\n",
      "         [ 3.9789e-06,  8.2117e-06,  9.6158e-06,  ...,  5.1178e-03,\n",
      "           5.0000e-01,  4.5490e-03],\n",
      "         [ 9.2332e-06,  1.0865e-05,  8.6916e-06,  ...,  8.0643e-03,\n",
      "           3.7500e-01,  6.6500e-03],\n",
      "         [ 1.1926e-05,  1.2412e-05,  6.9671e-06,  ...,  9.4176e-03,\n",
      "           3.7500e-01,  7.7999e-03]],\n",
      "\n",
      "        [[ 1.0405e-05,  1.1457e-05,  3.2397e-06,  ...,  7.5382e-02,\n",
      "           2.5000e-01,  6.0401e-02],\n",
      "         [ 1.2049e-05,  1.2430e-05,  3.4983e-06,  ...,  5.3568e-02,\n",
      "           3.1250e-01,  4.5106e-02],\n",
      "         [ 1.1008e-05,  1.1845e-05,  3.8801e-06,  ...,  5.5916e-02,\n",
      "           2.5000e-01,  4.7392e-02],\n",
      "         ...,\n",
      "         [-8.6555e-08,  4.2017e-06,  1.8854e-06,  ...,  4.1073e-02,\n",
      "           7.5000e-01,  3.5677e-02],\n",
      "         [-2.9818e-06,  4.3508e-06,  2.2192e-06,  ...,  4.3398e-02,\n",
      "           7.5000e-01,  3.9793e-02],\n",
      "         [-4.5085e-06,  4.7636e-06,  2.1097e-06,  ...,  4.3896e-02,\n",
      "           6.2500e-01,  3.9384e-02]]])\n",
      "phonemes\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "tensor([[22, 22, 22, 22, 22, 22, 22, 22, 22, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "         13, 13, 13, 13, 22, 22, 22, 14, 14, 14,  0,  0, 36, 36, 36, 36,  2,  2,\n",
      "          2,  2,  2,  2, 14, 14, 14, 14, 13, 13, 13, 13, 13, 36, 36, 36,  0,  0,\n",
      "          9,  9,  9,  9,  9, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "         27, 27, 36, 36, 36,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  5,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  8,  8,  8,  8,  8,  2,\n",
      "          2,  2,  2,  2,  2, 25, 25, 25, 25, 31, 31, 31,  0, 11, 11, 11, 11, 11,\n",
      "         11,  5,  5,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 20, 20, 20, 20, 20, 20, 28, 28, 28,\n",
      "         28, 28, 28, 28, 28, 28, 28, 14, 14, 14,  0,  0,  2,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0]])\n",
      "3\n",
      "label\n",
      "label <class 'list'>\n",
      "audio_feats\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "tensor([[[ -5.7061,  -5.0889,  -4.6871,  ..., -10.3345, -11.0622, -11.4832],\n",
      "         [ -5.8603,  -5.9838,  -6.1152,  ..., -10.6816, -11.3390, -11.4890],\n",
      "         [ -7.7347,  -8.3699, -10.8025,  ..., -11.2280, -11.4933, -11.5117],\n",
      "         ...,\n",
      "         [ -7.3045,  -6.9962,  -6.7410,  ..., -11.4174, -11.4825, -11.5117],\n",
      "         [ -7.8694,  -7.5704,  -7.3208,  ..., -11.0996, -11.4635, -11.5075],\n",
      "         [-10.2667,  -6.5823,  -5.8737,  ..., -11.1879, -11.4424, -11.5074]],\n",
      "\n",
      "        [[-10.8760, -10.6316, -10.4209,  ..., -11.5042, -11.5094, -11.5127],\n",
      "         [-11.2196, -10.8215, -10.5217,  ..., -11.5078, -11.5091, -11.5129],\n",
      "         [-11.3382, -11.2264, -11.1186,  ..., -11.4961, -11.5038, -11.5129],\n",
      "         ...,\n",
      "         [ -6.8552,  -7.4669,  -9.4778,  ..., -10.7971, -11.1097, -11.4990],\n",
      "         [ -6.1275,  -6.6540,  -7.8878,  ..., -10.7352, -11.0139, -11.4742],\n",
      "         [ -6.5507,  -6.9083,  -7.4783,  ..., -10.9779, -11.0103, -11.4818]]])\n",
      "audio_raw\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "tensor([[-5.7865e-02, -6.2384e-02, -6.7120e-02,  ..., -2.2453e-03,\n",
      "         -3.5495e-03, -2.4162e-03],\n",
      "        [ 8.7913e-05,  3.6364e-07, -6.7764e-05,  ...,  1.5092e-02,\n",
      "          1.9230e-02,  2.2645e-02]], dtype=torch.float64)\n",
      "eeg_raw\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "tensor([[[-1.8077e-06, -4.1242e-07,  4.0698e-07,  ..., -5.4821e-06,\n",
      "           1.3537e-04, -1.4428e-02],\n",
      "         [ 1.0549e-06,  5.7404e-07,  1.8931e-06,  ..., -1.9728e-06,\n",
      "           1.5610e-03,  3.2987e-02],\n",
      "         [ 6.0092e-06,  6.8455e-06,  6.7903e-06,  ...,  2.6101e-06,\n",
      "           1.3493e-03,  1.8966e-02],\n",
      "         ...,\n",
      "         [ 1.3468e-06,  1.0021e-06,  5.6751e-06,  ...,  4.5528e-06,\n",
      "           1.8769e-03, -3.7592e-02],\n",
      "         [ 1.2665e-06,  6.1320e-07,  5.2415e-06,  ...,  2.4125e-06,\n",
      "           1.3882e-03, -8.7363e-02],\n",
      "         [-1.2147e-06, -7.2182e-07,  1.7408e-06,  ...,  2.1881e-06,\n",
      "           8.1665e-04, -4.3961e-02]],\n",
      "\n",
      "        [[-2.0087e-06, -3.8332e-07,  1.3809e-06,  ..., -4.1430e-06,\n",
      "           1.8420e-03, -6.1120e-02],\n",
      "         [-4.9128e-06, -1.5075e-06, -1.8612e-07,  ..., -3.3828e-06,\n",
      "           6.0249e-03, -6.3686e-02],\n",
      "         [-7.6013e-06, -3.1609e-06, -2.0301e-06,  ..., -3.0429e-06,\n",
      "           8.4928e-03, -5.9353e-02],\n",
      "         ...,\n",
      "         [-1.0045e-05, -7.1505e-06, -8.5590e-06,  ..., -3.9775e-06,\n",
      "           3.7418e-03,  8.5265e-02],\n",
      "         [-6.1236e-06, -3.8232e-06, -4.6166e-06,  ..., -3.3144e-06,\n",
      "           3.0958e-03,  9.7189e-02],\n",
      "         [-4.6477e-06, -3.2431e-06, -3.6351e-06,  ..., -6.9885e-07,\n",
      "           1.6875e-03,  5.1649e-02]]], dtype=torch.float64)\n",
      "eeg_feats\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "tensor([[[ 1.7689e-07,  7.3233e-07,  3.8882e-06,  ...,  2.8791e-02,\n",
      "           8.7500e-01,  2.6411e-02],\n",
      "         [ 6.2909e-07,  1.4463e-06,  3.4516e-06,  ...,  3.5290e-02,\n",
      "           8.7500e-01,  3.1683e-02],\n",
      "         [ 1.4990e-06,  2.2497e-06,  3.3705e-06,  ...,  3.5470e-02,\n",
      "           8.1250e-01,  3.1490e-02],\n",
      "         ...,\n",
      "         [-6.2883e-06,  6.3464e-06,  1.8377e-06,  ...,  2.9710e-02,\n",
      "           5.0000e-01,  2.6678e-02],\n",
      "         [-5.8401e-06,  6.0466e-06,  1.4307e-06,  ...,  3.2448e-02,\n",
      "           6.8750e-01,  2.9036e-02],\n",
      "         [-4.5415e-06,  5.2236e-06,  1.4579e-06,  ...,  3.5818e-02,\n",
      "           6.8750e-01,  3.1653e-02]],\n",
      "\n",
      "        [[-1.1505e-05,  1.1947e-05,  3.0526e-06,  ...,  1.4521e-02,\n",
      "           1.2500e-01,  1.0394e-02],\n",
      "         [-1.3405e-05,  1.3447e-05,  2.9922e-06,  ...,  2.1776e-02,\n",
      "           1.2500e-01,  1.4313e-02],\n",
      "         [-1.3710e-05,  1.3719e-05,  1.6123e-06,  ...,  3.1786e-02,\n",
      "           1.8750e-01,  2.2883e-02],\n",
      "         ...,\n",
      "         [ 2.1381e-07,  4.1450e-06,  2.5771e-06,  ...,  3.7712e-02,\n",
      "           5.6250e-01,  3.3084e-02],\n",
      "         [-3.2229e-06,  5.1987e-06,  2.5203e-06,  ...,  3.7698e-02,\n",
      "           6.2500e-01,  3.2513e-02],\n",
      "         [-5.6969e-06,  6.3836e-06,  3.3587e-06,  ...,  4.1159e-02,\n",
      "           5.6250e-01,  3.5789e-02]]])\n",
      "phonemes\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "tensor([[13, 13, 13, 14, 14, 14,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  3,  5,  5,  5,  5,  5,  5,  5,  5,  0, 38, 38, 38, 38, 13, 13,\n",
      "         13, 13, 13, 13, 13, 13, 13, 26, 26, 26, 26, 26, 26, 26,  0,  0, 37, 37,\n",
      "         37, 37, 37, 37, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "         30, 30, 26, 26, 26, 26, 26, 26, 26, 26, 26,  2,  2,  2,  2,  2,  2,  2,\n",
      "         25, 25, 25,  0,  0, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 16, 16, 16,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10, 10,\n",
      "         10, 18, 18, 18, 18, 18, 18, 18, 18, 18,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21, 21, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 14, 14, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0, 13, 13, 13, 13, 13, 39, 39, 39, 39, 39, 39, 39, 33, 33, 33, 33,\n",
      "         33, 33, 33, 33]])\n",
      "4\n",
      "label\n",
      "label <class 'list'>\n",
      "audio_feats\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "tensor([[[ -7.2003,  -7.6912,  -8.7333,  ...,  -8.1441, -10.7942, -11.4363],\n",
      "         [ -6.3980,  -6.8944,  -7.9634,  ...,  -9.3109, -11.0578, -11.5030],\n",
      "         [ -7.5220,  -7.9950,  -8.9554,  ..., -10.5432, -11.2551, -11.5021],\n",
      "         ...,\n",
      "         [ -9.4401,  -9.3618,  -9.2747,  ..., -10.1534, -10.1818, -11.3141],\n",
      "         [ -9.1177,  -8.9125,  -8.7247,  ..., -11.1810, -11.1941, -11.4033],\n",
      "         [-10.8934, -10.7284, -10.5746,  ..., -11.3324, -11.1597, -11.4275]],\n",
      "\n",
      "        [[ -7.1075,  -7.1657,  -7.2155,  ..., -11.5082, -11.5003, -11.5117],\n",
      "         [-10.6813,  -7.1991,  -6.4936,  ..., -11.5085, -11.5112, -11.5127],\n",
      "         [ -7.4829,  -7.3693,  -7.2507,  ..., -11.5006, -11.5116, -11.5121],\n",
      "         ...,\n",
      "         [ -8.0569,  -6.6650,  -6.0785,  ..., -11.5083, -11.5072, -11.5099],\n",
      "         [ -7.8875,  -5.2985,  -4.6159,  ..., -11.4856, -11.5114, -11.5117],\n",
      "         [ -8.7455,  -5.1068,  -4.3989,  ..., -11.4737, -11.5065, -11.5117]]])\n",
      "audio_raw\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "tensor([[-0.0191, -0.0448, -0.0830,  ..., -0.0042, -0.0016, -0.0035],\n",
      "        [ 0.0016,  0.0012,  0.0011,  ...,  0.0919,  0.0721,  0.0528]],\n",
      "       dtype=torch.float64)\n",
      "eeg_raw\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "tensor([[[-1.0319e-06,  9.5842e-07,  2.4660e-06,  ..., -3.1160e-06,\n",
      "           2.0642e-03, -2.0942e-01],\n",
      "         [-1.1775e-06,  2.1545e-06,  4.2696e-06,  ..., -5.8558e-06,\n",
      "          -1.3476e-02, -5.1188e-01],\n",
      "         [-4.6852e-07,  4.3038e-06,  6.2494e-06,  ..., -8.1383e-06,\n",
      "          -1.3857e-03, -2.9485e-01],\n",
      "         ...,\n",
      "         [ 1.4077e-05,  1.0908e-05,  1.3185e-05,  ...,  1.0036e-05,\n",
      "          -1.0357e-04,  1.1826e-02],\n",
      "         [ 5.4771e-06,  4.2186e-06,  5.6857e-06,  ...,  2.8383e-06,\n",
      "           4.2595e-03,  2.9533e-02],\n",
      "         [ 3.0344e-06,  2.8182e-06,  2.9905e-06,  ...,  2.4235e-06,\n",
      "           1.5670e-03,  5.6678e-03]],\n",
      "\n",
      "        [[-1.3550e-07, -1.7317e-07, -1.1008e-07,  ..., -9.9003e-07,\n",
      "           1.5683e-03, -3.1377e-02],\n",
      "         [ 1.9478e-06,  2.3992e-06,  5.2481e-07,  ...,  2.8238e-06,\n",
      "           7.1343e-03, -3.7388e-02],\n",
      "         [ 2.9757e-06,  3.5498e-06,  1.0698e-07,  ...,  1.0262e-07,\n",
      "           1.0604e-02, -3.4369e-02],\n",
      "         ...,\n",
      "         [ 1.1384e-05,  8.1461e-06,  9.3786e-06,  ...,  1.6709e-05,\n",
      "           3.7963e-03, -2.6142e-07],\n",
      "         [ 9.1841e-06,  7.2225e-06,  7.7385e-06,  ...,  9.1804e-06,\n",
      "           2.3839e-03,  3.4213e-03],\n",
      "         [ 5.3104e-06,  5.1788e-06,  6.2294e-06,  ...,  6.1971e-06,\n",
      "           1.5008e-03,  2.2361e-03]]], dtype=torch.float64)\n",
      "eeg_feats\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "tensor([[[-1.2741e-06,  1.5240e-06,  3.0027e-06,  ...,  3.3736e-01,\n",
      "           6.2500e-01,  2.9923e-01],\n",
      "         [-8.0751e-07,  1.4559e-06,  3.7507e-06,  ...,  3.4723e-01,\n",
      "           6.8750e-01,  3.0780e-01],\n",
      "         [-1.5867e-08,  1.0708e-06,  4.0092e-06,  ...,  3.3952e-01,\n",
      "           6.2500e-01,  2.9515e-01],\n",
      "         ...,\n",
      "         [-7.0462e-06,  1.0638e-05,  4.5797e-06,  ...,  3.5711e-02,\n",
      "           5.0000e-01,  3.0081e-02],\n",
      "         [-4.0292e-07,  1.0650e-05,  6.2678e-06,  ...,  3.0416e-02,\n",
      "           3.7500e-01,  2.6011e-02],\n",
      "         [ 5.9986e-06,  1.0150e-05,  5.9702e-06,  ...,  3.0954e-02,\n",
      "           3.7500e-01,  2.6720e-02]],\n",
      "\n",
      "        [[ 2.0309e-06,  2.2182e-06,  1.6650e-06,  ...,  7.5503e-03,\n",
      "           3.1250e-01,  5.4426e-03],\n",
      "         [ 2.4315e-06,  2.5306e-06,  2.0656e-06,  ...,  4.4382e-03,\n",
      "           5.0000e-01,  3.4243e-03],\n",
      "         [ 1.9454e-06,  2.4543e-06,  1.9155e-06,  ...,  4.6832e-03,\n",
      "           5.6250e-01,  3.7016e-03],\n",
      "         ...,\n",
      "         [ 9.7113e-07,  6.3356e-06,  2.8779e-06,  ...,  2.3351e-03,\n",
      "           4.3750e-01,  2.0025e-03],\n",
      "         [ 6.1969e-06,  9.2130e-06,  4.1573e-06,  ...,  1.9282e-03,\n",
      "           4.3750e-01,  1.5314e-03],\n",
      "         [ 9.7527e-06,  1.0880e-05,  3.9775e-06,  ...,  1.6551e-03,\n",
      "           4.3750e-01,  1.3891e-03]]])\n",
      "phonemes\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "tensor([[ 2,  2,  2,  2,  2,  2,  2,  2, 25, 25, 25, 25, 25,  0,  0, 14, 14, 14,\n",
      "         14, 14, 14, 14, 14, 23, 23, 23, 23,  0,  0,  0,  6, 27, 27, 27, 27, 27,\n",
      "         27, 27, 27, 27, 27, 27, 27, 27, 27,  0, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "         24, 24, 24, 24, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "         35, 35, 35, 35,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "          0,  0, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 22, 22, 22,\n",
      "         22, 22, 13, 13, 13, 13, 13, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1, 14,\n",
      "         14, 14, 14, 14],\n",
      "        [ 0,  0,  0,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          5,  5,  5,  5,  5,  5,  5,  0,  0,  0,  0,  0, 16, 16, 16, 16, 16, 16,\n",
      "         18, 18, 18, 18, 18, 36, 36, 36, 36, 36, 36, 36, 36, 36,  0, 24, 24, 24,\n",
      "         24, 24, 24, 24, 18, 18, 18, 18, 18, 18, 35, 35, 35, 35, 35, 35, 35,  5,\n",
      "          5,  5,  5,  5,  5,  5,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1, 36, 36, 36, 36, 36, 36, 30, 30, 30,\n",
      "         30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 36, 36, 36, 36, 36,\n",
      "         36, 36, 36, 36]])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "i = 0\n",
    "for batch in train_dataloder:\n",
    "    print(i)\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            print(k)\n",
    "            print(k, v.shape, type(v))\n",
    "            print(v)\n",
    "        except:\n",
    "            print(k, type(v))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
