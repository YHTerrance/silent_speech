{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib import *\n",
    "from data_utils import combine_fixed_length, decollate_tensor\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from read_eeg import EEGDataset\n",
    "from data_utils import TextTransform\n",
    "import tqdm\n",
    "import jiwer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_dir = Path(\"/ocean/projects/cis240129p/shared/data/eeg_alice\")\n",
    "subjects_used = [\"S04\"]  # exclude 'S05' - less channels # , \"S13\", \"S19\"\n",
    "transform = TextTransform()\n",
    "ds = BrennanDataset(\n",
    "    root_dir=base_dir,\n",
    "    phoneme_dir=base_dir / \"phonemes\",\n",
    "    idx=\"S04\",\n",
    "    text_transform=transform,\n",
    "    phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n",
      "Subject S04 splits:\n",
      "  Train: 58 (69.0%)\n",
      "  Val: 13 (15.5%)\n",
      "  Test: 13 (15.5%)\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 58 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 9539\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 13 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 9472\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 13 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 6668\n"
     ]
    }
   ],
   "source": [
    "trainset, devset, testset = EEGDataset.from_subjects(\n",
    "    subjects=[\"S04\"],\n",
    "    # generated_subjects=generated_subjects,\n",
    "    base_dir=base_dir,\n",
    ")\n",
    "train_max_seq_len = trainset.verify_dataset()\n",
    "dev_max_seq_len = devset.verify_dataset()\n",
    "test_max_seq_len = testset.verify_dataset()\n",
    "\n",
    "max_seq_len = max(train_max_seq_len, dev_max_seq_len, test_max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from eeg_architecture import ResBlock\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer import TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class EEGModel(nn.Module):\n",
    "    def __init__(self, num_features, num_outs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            ResBlock(num_features, 768, 2),\n",
    "            ResBlock(768, 768, 2),\n",
    "            # ResBlock(768, 768, 2),\n",
    "        )\n",
    "        self.w_raw_in = nn.Linear(768, 768)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=768,\n",
    "            nhead=8,\n",
    "            relative_positional=True,\n",
    "            relative_positional_distance=100,\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, 6)\n",
    "        self.w_out = nn.Linear(768, num_outs)\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        # x shape is (batch, time, electrode)\n",
    "        x_raw = x_raw.transpose(1, 2)  # put channel before time for conv\n",
    "        x_raw = self.conv_blocks(x_raw)\n",
    "        x_raw = x_raw.transpose(1, 2)  # transpose back\n",
    "        x_raw = self.w_raw_in(x_raw)\n",
    "\n",
    "        x = x_raw\n",
    "\n",
    "        # put time first because transformers expect input int the shape (sequence length, batch size, feature dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return self.w_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(devset.text_transform.chars)\n",
    "model = EEGModel(devset.num_features, n_chars + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEGModel:\n\tsize mismatch for w_out.weight: copying a param with shape torch.Size([39, 768]) from checkpoint, the shape in current model is torch.Size([40, 768]).\n\tsize mismatch for w_out.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([40]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/recognition_model/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEGModel:\n\tsize mismatch for w_out.weight: copying a param with shape torch.Size([39, 768]) from checkpoint, the shape in current model is torch.Size([40, 768]).\n\tsize mismatch for w_out.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([40])."
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"models/recognition_model/model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=1,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "chars = \"\".join(testset.text_transform.chars) + \"_\"\n",
    "decoder = CTCBeamDecoder(\n",
    "    chars,\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    "    beam_width=20,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "# consine annealing scheduler\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=100)\n",
    "\n",
    "# eval mode\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# loop through epochs here\n",
    "with torch.no_grad():\n",
    "    for e in range(10):\n",
    "        losses = []\n",
    "        references = []\n",
    "        predictions = []\n",
    "        wers = []\n",
    "        for batch_i, example in tqdm.tqdm(\n",
    "            enumerate(dataloader), \"Train step\", disable=None\n",
    "        ):\n",
    "            X = combine_fixed_length(example[\"eeg_raw\"], 1000).float().to(device)\n",
    "            pred = model(X)\n",
    "            pred = F.log_softmax(pred, 2)\n",
    "            pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "            pred = nn.utils.rnn.pad_sequence(\n",
    "                decollate_tensor(pred, pred_lengths),\n",
    "                batch_first=False,\n",
    "                padding_value=trainset.text_transform.pad_token_id,\n",
    "            )\n",
    "            y = nn.utils.rnn.pad_sequence(\n",
    "                example[\"text_int\"],\n",
    "                batch_first=True,\n",
    "                padding_value=trainset.text_transform.pad_token_id,\n",
    "            ).to(device)\n",
    "            loss = F.ctc_loss(\n",
    "                pred, y, pred_lengths, example[\"text_int_lengths\"], blank=blank_id\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            # loss.backward()\n",
    "            pred = pred.permute(1, 0, 2)\n",
    "            beam_results, beam_scores, timesteps, out_lens = decoder.decode(\n",
    "                pred  # TODO: , seq_lens=example[\"text_int_lengths\"]\n",
    "            )\n",
    "            for i in range(len(y)):\n",
    "                target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "                # target_text = target_text.strip()\n",
    "                target_text = target_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "                references.append(target_text)\n",
    "                if i < len(beam_results):\n",
    "                    pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                    try:\n",
    "                        pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                        # pred_text = pred_text.strip()\n",
    "                        pred_text = pred_text.replace(\n",
    "                            trainset.text_transform.pad_token, \"\"\n",
    "                        )\n",
    "                    except:\n",
    "                        print(f\"!!!ERROR!!! batch idx: {batch_i}, i: {i}\")\n",
    "                        break\n",
    "                    predictions.append(pred_text)\n",
    "            torch.cuda.empty_cache()\n",
    "        train_loss = np.mean(losses)\n",
    "        train_wer = jiwer.wer(references, predictions)\n",
    "        print(f\"Epoch {e} train loss: {train_loss}, train wer: {train_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=1,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "chars = \"\".join(testset.text_transform.chars) + \"_\"\n",
    "decoder = CTCBeamDecoder(\n",
    "    chars,\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    "    beam_width=20,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "# consine annealing scheduler\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=100)\n",
    "\n",
    "# eval mode\n",
    "# model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# loop through epochs here\n",
    "# with torch.no_grad():\n",
    "model.train()\n",
    "for e in range(10):\n",
    "    losses = []\n",
    "    references = []\n",
    "    predictions = []\n",
    "    wers = []\n",
    "    for batch_i, example in tqdm.tqdm(\n",
    "        enumerate(dataloader), \"Train step\", disable=None\n",
    "    ):\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 1000).float().to(device)\n",
    "        pred = model(X)\n",
    "        pred = F.log_softmax(pred, 2)\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths),\n",
    "            batch_first=False,\n",
    "            # padding_value = n_chars-1,\n",
    "            padding_value=trainset.text_transform.pad_token_id,\n",
    "        )\n",
    "        y = nn.utils.rnn.pad_sequence(\n",
    "            example[\"text_int\"],\n",
    "            batch_first=True,\n",
    "            # padding_value = n_chars-1,\n",
    "            padding_value=trainset.text_transform.pad_token_id,\n",
    "        ).to(device)\n",
    "        loss = F.ctc_loss(\n",
    "            pred, y, pred_lengths, example[\"text_int_lengths\"], blank=blank_id\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        pred = pred.permute(1, 0, 2)\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(\n",
    "            pred  # TODO: , seq_lens=example[\"text_int_lengths\"]\n",
    "        )\n",
    "        for i in range(len(y)):\n",
    "            target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "            # target_text = target_text.strip()\n",
    "            target_text = target_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "            references.append(target_text)\n",
    "            if i < len(beam_results):\n",
    "                pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                try:\n",
    "                    pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                    # pred_text = pred_text.strip()\n",
    "                    pred_text = pred_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "                except:\n",
    "                    print(f\"!!!ERROR!!! batch idx: {batch_i}, i: {i}\")\n",
    "                    break\n",
    "                predictions.append(pred_text)\n",
    "        if (batch_i + 1) % 2 == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss = np.mean(losses)\n",
    "    train_wer = jiwer.wer(references, predictions)\n",
    "    print(f\"Epoch {e} train loss: {train_loss}, train wer: {train_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her',\n",
       " 'she found herself in a long low hall which was lit up by a row of lamps hanging from the roof nabakatokia bagabornabou cacahuamilpa abecedarian cabalistically cadaverousness lafayette chaboisseau bababalouk sabachthani abacadabra eatanswill babalatchi bababalouk hadadrimmon',\n",
       " 'it ll never do to ask perhaps i shall see it written up somewhere ekateringofsky abdalmalek dabulamanzi babalatchi alcacarquivir arbalestriers balachulish bababalouk babebibobubybaeboe babebibobubybaeboe abandonment habareskul academicianship babalatchi adachigahara academicianship jagadananda cabalistically',\n",
       " 'i hope they ll remember her saucer of milk at tea time babebibobubybaeboe babebibobubybaeboe sbarovitch babalatchi academicianship anachronistically gabardines babalatchi catachrestically bababalouk babalatchi gabrilowitsch babalatchi academicianship hadatchishi badakhshan gabardines academicianship',\n",
       " 'oh my ears and whiskers how late it s getting she was close behind it when she turned the corner but the rabbit was no longer to be seen sabachthani academicianship cabalistically labanyalekha cacahuamilpa bababalouk babebibobubybaeboe darachanarvan achaemenidae babamarishi',\n",
       " 'to be sure this generally happens when one eats caernarvonshire gabardines babalatchi cadaverousness canandaigua bababalouk bagabornabou cacahuamilpa babalatchi hadadrimmon bhamaniwallah haranguing abandonment apalachicola beaconsfield calabashes ragamuffins kabibonokka macadamization',\n",
       " 'here and there she saw maps and pictures hung up on pegs she took down a jar from one of the shelves as she passed it was labelled orange marmalade exageration beachcombers radamanthus babalatchi haberdashery rajahmahendri nabakatokia macadamization cabalistically jafanatapan',\n",
       " 'it s almost certain to disagree with you sooner or later abacadabra bababalouk babalatchi achaemenidae achaemenidae racecourses bicarbonate academicianship babalatchi academicianship darachanarvan madamoiselle cacaracamouchen cacaracamouchen haberdashery bagabornabou cabalistically babalatchi bhamaniwallah babamarishi']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her',\n",
       " 'she found herself in a long low hall which was lit up by a row of lamps hanging from the roofaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'it ll never do to ask perhaps i shall see it written up somewhereaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'i hope they ll remember her saucer of milk at tea timeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'oh my ears and whiskers how late it s getting she was close behind it when she turned the corner but the rabbit was no longer to be seenaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'to be sure this generally happens when one eats cakeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'here and there she saw maps and pictures hung up on pegs she took down a jar from one of the shelves as she passed it was labelled orange marmaladeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'it s almost certain to disagree with you sooner or lateraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=devset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=8,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "decoder = CTCBeamDecoder(\n",
    "    devset.text_transform.chars + \"_\",\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "lr_sched = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[125, 150, 175], gamma=0.5\n",
    ")\n",
    "# eval mode\n",
    "model.eval()\n",
    "# loop through epochs here\n",
    "losses = []\n",
    "references = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_i, example in tqdm.tqdm(\n",
    "        enumerate(dataloader), \"Train step\", disable=None\n",
    "    ):\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 5000).float().to(device)\n",
    "        pred = model(X)\n",
    "        pred = F.log_softmax(pred, 2)\n",
    "\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred_pad = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths), batch_first=False\n",
    "        )\n",
    "\n",
    "        y = nn.utils.rnn.pad_sequence(example[\"text_int\"], batch_first=True).to(\n",
    "            device\n",
    "        )  # TODO: padding_value\n",
    "        loss = F.ctc_loss(\n",
    "            pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        pred_pad = pred_pad.permute(1, 0, 2)\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred_pad)\n",
    "        for i in range(len(y)):\n",
    "            target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "            references.append(target_text)\n",
    "            if i < len(beam_results):\n",
    "                pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                try:\n",
    "                    pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                except:\n",
    "                    print(f\"batch idx: {batch_i}, i: {i}\")\n",
    "                    break\n",
    "                predictions.append(pred_text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1250, 38]), torch.Size([8, 309]), torch.Size([8, 2368, 38]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, y.shape, pred_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esemplastic babebibobubybaeboe babebibobubybaeboe icaromenippus balancing garamapingwe academicianship cabalistically achaemenidae babalatchi academicians macadamization cacaracamouchen haberdashery hadadrimmon cabalistically araucanians achaemenidae cacaracamouchen academicians achaemenidae arabesques babalatchi cacahuamilpa cabalistically academicians abhandlungen mablethorpe cabalistically babalatchi bababalouk sagaciously abhandlungen',\n",
       " 'deinde astonished eyatonkawee capabilities babalatchi bababalouk gabardines cabalistically chakamankabudibaba cadaverousness babalatchi sagaciously bagabornabou alcacarquivir tablecloths eachdaireachd gcalekaland babebibobubybaeboe cabalistically dabulamanzi bagabornabou academicianship',\n",
       " 'ebenezer gablehurst falcinellus cabalistically anabaptists jadakweniyosaon capabilities damanarkist cacahuamilpa babebibobubybaeboe lagadigadeou jadakweniyosaon bibativeness babalatchi anabaptists afanassievna eachdaireachd nayakoghstonde adachigahara adachigahara cachemirian academicians babebibobubybaeboe tanacharisson bhagavadgita lagadigadeou babalatchi babalatchi cabalistically blagadenztoiga bagabornabou',\n",
       " 'rhodomont elsie in someat the esterel are neater elapsed even therese seated oletta let entendait',\n",
       " 'i loewestein hareton elephantiasis cacaracamouchen abhandlungen cabalistically labanyalekha beachcombers achaemenidae babalatchi bagabornabou babalatchi kaiakahinalii nabakatokia hackelberend dabulamanzi cacahuamilpa gabardines cadaverousness academicians beachcombers bababalouk macadamization cacaracamouchen albatrosses cacahuamilpa hadadrimmon',\n",
       " 'einerhalben hadadrimmon academicians jadakweniyosaon cadaverousness fabrications eavesdropping academicianship cabalistically academicians adachigahara cabalistically babalatchi labanyalekha hamadryades bababalouk macadamization camaralzaman cabalistically abandonment achaemenidae beachcombers badakhshan macadamization eachdaireachd hadatchishi macadamization sabachthani babebibobubybaeboe',\n",
       " 'eternities gabardines academicians cabalistically academicians bababalouk dabulamanzi academicianship damanarkist academicians madamoiselle cabalistically babebibobubybaeboe bamangwato adachigahara bababalouk tabachetti cabalistically adachigahara babebibobubybaeboe cabalistically cabalistically academicians bagabornabou academicians cacaracamouchen gabardines cabalistically babamarishi',\n",
       " 'e u t is consoled nonappearance neither a tsernoyevitcha academicianship academicianship cacahuamilpa babalatchi jadakweniyosaon cacaracamouchen babalatchi cabalistically adairsville bababalouk cacaracamouchen bababalouk cacahuamilpa bababalouk']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must be getting somewhere near the center of the earthaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'either the well was very deep or she fell very slowly for she had plenty of time as she went down to look about her and to wonder what was going to happen nextaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'please maam is this new zealand or australia and she tried to curtsey as she spoke fancy curtseying as you re falling through the air do you think you could manage it and what an ignorant little girl she ll think me for asking noaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'the poor little thing sat down and cried come there s no use crying like that said alice to herself rather sharply i advise you to leave off this minute she generally gave herself very good advice though she very seldom followed it and sometimes she scolded herself so severely as to bring tears into her eyes',\n",
       " 'she felt that she was dozing off and had just begun to dream that she was walking hand in hand with dinahaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'and the white rabbit was still in sight hurrying downaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'she felt a little nervous about this for it might end you knowaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'the rabbit hole went straight on like a tunnel for some way and then dipped suddenly down so suddenly that alice had not a moment to think about stopping herself before she found herself falling down a very deep wellaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeer theeeeereeeeeeeeeeeee was nothing so veeeeeeeeeeeeeeeeeeeeeeeeeery reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeemarrkableeeeeeeeeeeeeeeeeeeeeeeeeeeee in that nor did aliceeeeeeeeeeeeeeeeeeeeeee think it  so veery much  out of theeeeeeeeeeeeee way  to hear the rab_bit say to itself oh  deeeeeeeeeeeear oh deeeeeeeeeeeeeeeeeeeeeeeeeeeear i sshal_l beeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee       latteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee wheeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeen   sheeeeeeeeeeeeeeeeeeeeeeeeeeeee thought it oveeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeer afteeeeeeeeeeeeeeeeeeeeerwards it oc_cur_reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeed to heeeeeeeeeeeeeeeeeeeeeeeer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transform = TextTransform()\n",
    "text_transform.chars += \"_\"\n",
    "text_transform.int_to_text(p1.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it ove'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)\n",
    "pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()\n",
    "pred_text = testset.text_transform.int_to_text(pred_int)\n",
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 1250])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13043478260869565"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_txt = testset.text_transform.int_to_text(y[0].cpu().numpy())\n",
    "jiwer.wer(target_txt, pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "batch_size = 2\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "decoder = CTCBeamDecoder(\n",
    "    devset.text_transform.chars + \"_\",\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    ")\n",
    "model.eval()\n",
    "references = []\n",
    "predictions = []\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for example in tqdm.tqdm(dataloader, \"Evaluate\", disable=None):\n",
    "        # X = example[\"eeg_raw\"][0].float().to(device)\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 5000).float().to(device)\n",
    "        pred = F.log_softmax(model(X), -1)\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred_pad = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths), batch_first=False\n",
    "        )\n",
    "\n",
    "        # y = nn.utils.rnn.pad_sequence(example[\"text_int\"], batch_first=True).to(device)\n",
    "        y = torch.cat(example[\"text_int\"]).to(device)\n",
    "        loss = F.ctc_loss(\n",
    "            pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)\n",
    "        pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()\n",
    "\n",
    "        pred_text = testset.text_transform.int_to_text(pred_int)\n",
    "        target_text = testset.text_transform.clean_text(example[\"labels\"][0])\n",
    "\n",
    "        references.append(target_text)\n",
    "        predictions.append(pred_text)\n",
    "wer = jiwer.wer(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_pad: torch.Size([761, 2, 38])\n",
      "pred.shape: torch.Size([1, 1250, 38])\n",
      "y.shape: torch.Size([155])\n",
      "pred_lengths: [248, 761]\n",
      "n_chars: 37\n",
      "loss: 2.801051139831543\n"
     ]
    }
   ],
   "source": [
    "print(f\"pred_pad: {pred_pad.shape}\")\n",
    "print(f\"pred.shape: {pred.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"pred_lengths: {pred_lengths}\")\n",
    "print(f\"n_chars: {n_chars}\")\n",
    "loss = F.ctc_loss(pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars)\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetota',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetaheite',\n",
       " 'tetootne',\n",
       " 'teetotalis',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetota',\n",
       " 'teetota',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetotale',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetotalis',\n",
       " 'etteniot',\n",
       " 'tetootne',\n",
       " 'teetotale',\n",
       " 'etonensis',\n",
       " 'tetootne',\n",
       " 'tottontai']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37]\n"
     ]
    }
   ],
   "source": [
    "pred_exp = torch.exp(pred)  # Convert log_probs back to probabilities\n",
    "pred_max = pred_exp.argmax(dim=-1)  # Get the most likely token at each time step\n",
    "print(pred_max.squeeze().tolist())  # Inspect the token indices over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred_pad)\n",
    "beam_results_o, beam_scores_o, timesteps_o, out_lens_o = decoder.decode(pred)\n",
    "# pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([761, 100, 1]), torch.Size([1, 100, 1250]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape, beam_results_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1250, 38])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_int = beam_results_o[0, 0, : out_lens_o[:, 0]].tolist()  # out_lens_o[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset.text_transform.int_to_text(pred_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([761, 100, 1]), 120)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape, len(example[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8002, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.ctc_loss(pred_prd, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eeg_raw', 'labels', 'lengths', 'text_int', 'text_int_lengths'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tetootne'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789 '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.text_transform.chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset.text_transform.chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_architecture import EEGModel\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from data_utils import combine_fixed_length, decollate_tensor\n",
    "from IPython.core.debugger import Pdb\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "l2 = 1e-5\n",
    "n_epochs = 2\n",
    "learning_rate_warmup = 100\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(devset, batch_size=1)\n",
    "for example in tqdm.tqdm(dataloader, \"Evaluate\", disable=None):\n",
    "    target = example[\"label\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MUST BE GETTING SOMEWHERE NEAR THE CENTER OF THE EARTH'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    }
   ],
   "source": [
    "ds_brennan = BrennanDataset(\n",
    "    root_dir=base_dir,\n",
    "    phoneme_dir=base_dir / \"phonemes\",\n",
    "    idx=\"S04\",\n",
    "    phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice\n",
      "dict_keys(['label', 'audio_feats', 'audio_raw', 'eeg_raw', 'eeg_feats', 'phonemes'])\n",
      "(104, 128) (159, 300) 104\n"
     ]
    }
   ],
   "source": [
    "item2 = ds_brennan[0]\n",
    "print(item2.keys())\n",
    "print(item2[\"audio_feats\"].shape, item2[\"eeg_feats\"].shape, len(item2[\"phonemes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1703, Test dataset length: 426\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(subjects_used, base_dir)\n",
    "\n",
    "print(\n",
    "    f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     A custom collate function that handles different types of data in a batch.\n",
    "#     It dynamically creates batches by converting arrays or lists to tensors and\n",
    "#     applies padding to variable-length sequences.\n",
    "#     \"\"\"\n",
    "#     batch_dict = {}\n",
    "#     for key in batch[0].keys():\n",
    "#         batch_items = [item[key] for item in batch]\n",
    "#         if isinstance(batch_items[0], np.ndarray) or isinstance(\n",
    "#             batch_items[0], torch.Tensor\n",
    "#         ):\n",
    "#             if isinstance(batch_items[0], np.ndarray):\n",
    "#                 batch_items = [torch.tensor(b) for b in batch_items]\n",
    "#             if len(batch_items[0].shape) > 0:\n",
    "#                 batch_dict[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "#                     batch_items, batch_first=True  # pad with zeros\n",
    "#                 )\n",
    "#             else:\n",
    "#                 batch_dict[key] = torch.stack(batch_items)\n",
    "#         else:\n",
    "#             batch_dict[key] = batch_items\n",
    "\n",
    "#     return batch_dict\n",
    "\n",
    "\n",
    "train_dataloder = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label <class 'str'>\n",
      "audio_feats (104, 128) <class 'numpy.ndarray'>\n",
      "audio_raw (16735,) <class 'numpy.ndarray'>\n",
      "eeg_raw (520, 62) <class 'numpy.ndarray'>\n",
      "eeg_feats (159, 310) <class 'numpy.ndarray'>\n",
      "phonemes (104,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k, v in item.items():\n",
    "    try:\n",
    "        print(k, v.shape, type(v))\n",
    "    except:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"eeg_raw\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "1\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "2\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "3\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "4\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "i = 0\n",
    "for batch in train_dataloder:\n",
    "    print(i)\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            print(k, v.shape, type(v))\n",
    "        except:\n",
    "            print(k, type(v))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
