{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib import *\n",
    "from data_utils import combine_fixed_length, decollate_tensor\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from read_eeg import EEGDataset\n",
    "from data_utils import TextTransform, TextTransformOrig\n",
    "import tqdm\n",
    "import jiwer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_dir = Path(\"/ocean/projects/cis240129p/shared/data/eeg_alice\")\n",
    "subjects_used = [\"S04\"]  # exclude 'S05' - less channels # , \"S13\", \"S19\"\n",
    "transform = TextTransform()\n",
    "# ds = BrennanDataset(\n",
    "#     root_dir=base_dir,\n",
    "#     phoneme_dir=base_dir / \"phonemes\",\n",
    "#     idx=\"S04\",\n",
    "#     text_transform=transform,\n",
    "#     phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "#     debug=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\n",
    "    \"S01\",\n",
    "    \"S03\",\n",
    "    \"S04\",\n",
    "    # \"S05\", missing one channel\n",
    "    \"S08\",\n",
    "    \"S11\",\n",
    "    \"S12\",\n",
    "    \"S13\",\n",
    "    \"S16\",\n",
    "    \"S17\",\n",
    "    \"S18\",\n",
    "    \"S19\",\n",
    "    \"S22\",\n",
    "    \"S26\",\n",
    "    \"S36\",\n",
    "    \"S37\",\n",
    "    # \"S38\", missing one channel\n",
    "    \"S40\",\n",
    "    \"S41\",\n",
    "    \"S42\",\n",
    "    \"S44\",\n",
    "    \"S48\",\n",
    "]\n",
    "# trainset, devset, testset = EEGDataset.from_subjects(\n",
    "#     subjects=subjects,\n",
    "#     # generated_subjects=generated_subjects,\n",
    "#     base_dir=base_dir,\n",
    "#     train_ratio=0.8,\n",
    "#     dev_ratio=0.1,\n",
    "#     test_ratio=0.1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S01.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 366524  =      0.000 ...   733.048 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S03.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 367299  =      0.000 ...   734.598 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S08.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 369574  =      0.000 ...   739.148 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S11.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 369574  =      0.000 ...   739.148 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S12.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 365124  =      0.000 ...   730.248 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S13.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368274  =      0.000 ...   736.548 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S16.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 366924  =      0.000 ...   733.848 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S17.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 365649  =      0.000 ...   731.298 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S18.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 370149  =      0.000 ...   740.298 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S19.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 373374  =      0.000 ...   746.748 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S22.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 365024  =      0.000 ...   730.048 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S26.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 363324  =      0.000 ...   726.648 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S36.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 364074  =      0.000 ...   728.148 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S37.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 363724  =      0.000 ...   727.448 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S40.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 365624  =      0.000 ...   731.248 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S41.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 365024  =      0.000 ...   730.048 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S42.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 364099  =      0.000 ...   728.198 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S44.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 364499  =      0.000 ...   728.998 secs...\n",
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S48.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 364224  =      0.000 ...   728.448 secs...\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_data = []  # (subject_id, idx, sample)\n",
    "sentence_to_indices = defaultdict(list)  # sentence -> list of indices in all_data\n",
    "text_transform = TextTransformOrig()\n",
    "preload_dataset = {}\n",
    "\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "\n",
    "    dataset = BrennanDataset(\n",
    "        text_transform=text_transform,\n",
    "        root_dir=base_dir,\n",
    "        phoneme_dir=base_dir / \"phonemes\",\n",
    "        idx=subject,\n",
    "        phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    )\n",
    "    preload_dataset[subject_id] = dataset\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        sentence = sample[\"label\"]\n",
    "        all_data.append((subject_id, idx, sample))\n",
    "        sentence_to_indices[sentence].append(len(all_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, dev_indices, test_indices = (\n",
    "    defaultdict(list),\n",
    "    defaultdict(list),\n",
    "    defaultdict(list),\n",
    ")\n",
    "for sentence, indices in sentence_to_indices.items():\n",
    "    # stratify by sentences\n",
    "    if len(indices) < 10:\n",
    "        continue\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=1)\n",
    "    dev_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=1)\n",
    "    for idx in train_idx:\n",
    "        subject_id, _, _ = all_data[idx]\n",
    "        train_indices[subject_id].append(idx)\n",
    "    for idx in dev_idx:\n",
    "        subject_id, _, _ = all_data[idx]\n",
    "        dev_indices[subject_id].append(idx)\n",
    "    for idx in test_idx:\n",
    "        subject_id, _, _ = all_data[idx]\n",
    "        test_indices[subject_id].append(idx)\n",
    "trainsets, devsets, testsets = [], [], []\n",
    "for subject_id, subject in enumerate(subjects):\n",
    "    trainsets.append(Subset(preload_dataset[subject_id], train_indices[subject_id]))\n",
    "    devsets.append(Subset(preload_dataset[subject_id], dev_indices[subject_id]))\n",
    "    testsets.append(Subset(preload_dataset[subject_id], test_indices[subject_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_freq = {}\n",
    "for i in range(len(trainset)):\n",
    "    sentence = trainset[i][\"label\"]\n",
    "    if sentence not in sentence_freq:\n",
    "        sentence_freq[sentence] = {\"train\": 0, \"dev\": 0}\n",
    "    sentence_freq[sentence][\"train\"] += 1\n",
    "\n",
    "for i in range(len(devset)):\n",
    "    sentence = devset[i][\"label\"]\n",
    "    if sentence not in sentence_freq:\n",
    "        sentence_freq[sentence] = {\"train\": 0, \"dev\": 0}\n",
    "    sentence_freq[sentence][\"dev\"] += 1\n",
    "\n",
    "for i in range(len(testset)):\n",
    "    sentence = testset[i][\"label\"]\n",
    "    if sentence not in sentence_freq:\n",
    "        sentence_freq[sentence] = {\"train\": 0, \"dev\": 0}\n",
    "    sentence_freq[sentence][\"test\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>THE POOR LITTLE THING SAT DOWN AND CRIED COME THERE S NO USE CRYING LIKE THAT SAID ALICE TO HERSELF RATHER SHARPLY I ADVISE YOU TO LEAVE OFF THIS MINUTE SHE GENERALLY GAVE HERSELF VERY GOOD ADVICE THOUGH SHE VERY SELDOM FOLLOWED IT AND SOMETIMES SHE SCOLDED HERSELF SO SEVERELY AS TO BRING TEARS INTO HER EYES</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND THE WHITE RABBIT WAS STILL IN SIGHT HURRYING DOWN</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUT ALAS EITHER THE LOCKS WERE TOO LARGE OR THE KEY WAS TOO SMALL BUT AT ANY RATE IT WOULD NOT OPEN ANY OF THEM HOWEVER ON THE SECOND TIME ROUND SHE CAME UPON A LOW CURTAIN SHE HAD NOT NOTICED BEFORE</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE RABBIT HOLE WENT STRAIGHT ON LIKE A TUNNEL FOR SOME WAY AND THEN DIPPED SUDDENLY DOWN SO SUDDENLY THAT ALICE HAD NOT A MOMENT TO THINK ABOUT STOPPING HERSELF BEFORE SHE FOUND HERSELF FALLING DOWN A VERY DEEP WELL</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLEASE MAAM IS THIS NEW ZEALAND OR AUSTRALIA AND SHE TRIED TO CURTSEY AS SHE SPOKE FANCY CURTSEYING AS YOU RE FALLING THROUGH THE AIR DO YOU THINK YOU COULD MANAGE IT AND WHAT AN IGNORANT LITTLE GIRL SHE LL THINK ME FOR ASKING NO</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    train  dev\n",
       "THE POOR LITTLE THING SAT DOWN AND CRIED COME T...     20    0\n",
       "AND THE WHITE RABBIT WAS STILL IN SIGHT HURRYIN...     20    0\n",
       "BUT ALAS EITHER THE LOCKS WERE TOO LARGE OR THE...     18    2\n",
       "THE RABBIT HOLE WENT STRAIGHT ON LIKE A TUNNEL ...     20    0\n",
       "PLEASE MAAM IS THIS NEW ZEALAND OR AUSTRALIA AN...     18    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataframe\n",
    "df = pd.DataFrame(sentence_freq).T\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique sentences: 83\n",
      "# only in trainset: 70\n",
      "# only in devset: 7\n",
      "# in both train and devset: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of unique sentences: {len(df)}\")\n",
    "print(f\"# only in trainset: {len(df[df['dev'] == 0])}\")\n",
    "print(f\"# only in devset: {len(df[df['train'] == 0])}\")\n",
    "print(f\"# in both train and devset: {len(df[(df['train'] > 0) & (df['dev'] > 0)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/ocean/projects/cis240129p/shared/conda/envs/new_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/ocean/projects/cis240129p/shared/conda/envs/new_env/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/ocean/projects/cis240129p/shared/conda/envs/new_env/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count how many times sentences only appear in trainset times the frequency\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# of the sentence in the trainset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/ocean/projects/cis240129p/shared/conda/envs/new_env/lib/python3.8/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/ocean/projects/cis240129p/shared/conda/envs/new_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "# Count how many times sentences only appear in trainset times the frequency\n",
    "# of the sentence in the trainset\n",
    "\n",
    "df[df[\"test\"] == 0][\"train\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 160)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"train\"] == 0][\"dev\"].sum(), df[\"dev\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SO ALICE SOON BEGAN TALKING AGAIN DINAH LL MISS ME VERY MUCH TONIGHT I SHOULD THINK DINAH WAS THE CAT</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EITHER THE WELL WAS VERY DEEP OR SHE FELL VERY SLOWLY FOR SHE HAD PLENTY OF TIME AS SHE WENT DOWN TO LOOK ABOUT HER AND TO WONDER WHAT WAS GOING TO HAPPEN NEXT</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOR FEAR OF KILLING SOMEBODY SO SHE MANAGED TO PUT IT INTO ONE OF THE CUPBOARDS AS SHE FELL PAST IT</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DINAH MY DEAR I WISH YOU WERE DOWN HERE WITH ME THERE ARE NO MICE IN THE AIR I M AFRAID BUT YOU MIGHT CATCH A BAT AND THAT S VERY LIKE A MOUSE</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUST BE GETTING SOMEWHERE NEAR THE CENTER OF THE EARTH</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN ANOTHER MOMENT DOWN WENT ALICE AFTER IT NEVER ONCE CONSIDERING HOW IN THE WORLD SHE WAS TO GET OUT AGAIN</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND SHE TRIED TO FANCY WHAT THE FLAME OF A CANDLE IS LIKE AFTER THE CANDLE IS BLOWN OUT</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    train  dev\n",
       "SO ALICE SOON BEGAN TALKING AGAIN DINAH LL MISS...      0   18\n",
       "EITHER THE WELL WAS VERY DEEP OR SHE FELL VERY ...      0   18\n",
       "FOR FEAR OF KILLING SOMEBODY SO SHE MANAGED TO ...      0   20\n",
       "DINAH MY DEAR I WISH YOU WERE DOWN HERE WITH ME...      0   18\n",
       "MUST BE GETTING SOMEWHERE NEAR THE CENTER OF TH...      0   20\n",
       "IN ANOTHER MOMENT DOWN WENT ALICE AFTER IT NEVE...      0   20\n",
       "AND SHE TRIED TO FANCY WHAT THE FLAME OF A CAND...      0    2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"train\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BUT ALAS EITHER THE LOCKS WERE TOO LARGE OR THE KEY WAS TOO SMALL BUT AT ANY RATE IT WOULD NOT OPEN ANY OF THEM HOWEVER ON THE SECOND TIME ROUND SHE CAME UPON A LOW CURTAIN SHE HAD NOT NOTICED BEFORE</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHE FOUND HERSELF IN A LONG LOW HALL WHICH WAS LIT UP BY A ROW OF LAMPS HANGING FROM THE ROOF</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT LL NEVER DO TO ASK PERHAPS I SHALL SEE IT WRITTEN UP SOMEWHERE</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIRST HOWEVER SHE WAITED FOR A FEW MINUTES TO SEE IF SHE WAS GOING TO SHRINK ANY FURTHER</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND SAYING TO HER VERY EARNESTLY NOW DINAH TELL ME THE TRUTH DID YOU EVER EAT A BAT WHEN SUDDENLY THUMP THUMP DOWN SHE CAME UPON A HEAP OF STICKS AND DRY LEAVES</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOWEVER THIS BOTTLE WAS NOT MARKED POISON SO ALICE VENTURED TO TASTE IT AND FINDING IT VERY NICE IT HAD IN FACT A SORT OF MIXED FLAVOR OF CHERRY TART CUSTARD PINEAPPLE ROAST TURKEY TOFFEE AND HOT BUTTERED TOAST</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    train  dev\n",
       "BUT ALAS EITHER THE LOCKS WERE TOO LARGE OR THE...     18    2\n",
       "SHE FOUND HERSELF IN A LONG LOW HALL WHICH WAS ...     18    2\n",
       "IT LL NEVER DO TO ASK PERHAPS I SHALL SEE IT WR...     18    2\n",
       "FIRST HOWEVER SHE WAITED FOR A FEW MINUTES TO S...     18    2\n",
       "AND SAYING TO HER VERY EARNESTLY NOW DINAH TELL...      2   18\n",
       "HOWEVER THIS BOTTLE WAS NOT MARKED POISON SO AL...      2   18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"train\"] > 0) & (df[\"dev\"] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n",
      "Subject S04 splits:\n",
      "  Train: 58 (69.0%)\n",
      "  Val: 13 (15.5%)\n",
      "  Test: 13 (15.5%)\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 58 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 9539\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 13 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 9472\n",
      "Verifying dataset...\n",
      "Dataset verification complete. 13 samples checked.\n",
      "EEG feature dimensions: 60\n",
      "Longest sequence length: 6668\n"
     ]
    }
   ],
   "source": [
    "trainset, devset, testset = EEGDataset.from_subjects(\n",
    "    subjects=[\"S04\"],\n",
    "    # generated_subjects=generated_subjects,\n",
    "    base_dir=base_dir,\n",
    ")\n",
    "train_max_seq_len = trainset.verify_dataset()\n",
    "dev_max_seq_len = devset.verify_dataset()\n",
    "test_max_seq_len = testset.verify_dataset()\n",
    "\n",
    "max_seq_len = max(train_max_seq_len, dev_max_seq_len, test_max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from eeg_architecture import ResBlock\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer import TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class EEGModel(nn.Module):\n",
    "    def __init__(self, num_features, num_outs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            ResBlock(num_features, 768, 2),\n",
    "            ResBlock(768, 768, 2),\n",
    "            # ResBlock(768, 768, 2),\n",
    "        )\n",
    "        self.w_raw_in = nn.Linear(768, 768)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=768,\n",
    "            nhead=8,\n",
    "            relative_positional=True,\n",
    "            relative_positional_distance=100,\n",
    "            dim_feedforward=3072,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, 6)\n",
    "        self.w_out = nn.Linear(768, num_outs)\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        # x shape is (batch, time, electrode)\n",
    "        x_raw = x_raw.transpose(1, 2)  # put channel before time for conv\n",
    "        x_raw = self.conv_blocks(x_raw)\n",
    "        x_raw = x_raw.transpose(1, 2)  # transpose back\n",
    "        x_raw = self.w_raw_in(x_raw)\n",
    "\n",
    "        x = x_raw\n",
    "\n",
    "        # put time first because transformers expect input int the shape (sequence length, batch size, feature dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return self.w_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = len(devset.text_transform.chars)\n",
    "model = EEGModel(devset.num_features, n_chars + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEGModel:\n\tsize mismatch for w_out.weight: copying a param with shape torch.Size([39, 768]) from checkpoint, the shape in current model is torch.Size([40, 768]).\n\tsize mismatch for w_out.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([40]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/recognition_model/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEGModel:\n\tsize mismatch for w_out.weight: copying a param with shape torch.Size([39, 768]) from checkpoint, the shape in current model is torch.Size([40, 768]).\n\tsize mismatch for w_out.bias: copying a param with shape torch.Size([39]) from checkpoint, the shape in current model is torch.Size([40])."
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"models/recognition_model/model.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=1,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "chars = \"\".join(testset.text_transform.chars) + \"_\"\n",
    "decoder = CTCBeamDecoder(\n",
    "    chars,\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    "    beam_width=20,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "# consine annealing scheduler\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=100)\n",
    "\n",
    "# eval mode\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# loop through epochs here\n",
    "with torch.no_grad():\n",
    "    for e in range(10):\n",
    "        losses = []\n",
    "        references = []\n",
    "        predictions = []\n",
    "        wers = []\n",
    "        for batch_i, example in tqdm.tqdm(\n",
    "            enumerate(dataloader), \"Train step\", disable=None\n",
    "        ):\n",
    "            X = combine_fixed_length(example[\"eeg_raw\"], 1000).float().to(device)\n",
    "            pred = model(X)\n",
    "            pred = F.log_softmax(pred, 2)\n",
    "            pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "            pred = nn.utils.rnn.pad_sequence(\n",
    "                decollate_tensor(pred, pred_lengths),\n",
    "                batch_first=False,\n",
    "                padding_value=trainset.text_transform.pad_token_id,\n",
    "            )\n",
    "            y = nn.utils.rnn.pad_sequence(\n",
    "                example[\"text_int\"],\n",
    "                batch_first=True,\n",
    "                padding_value=trainset.text_transform.pad_token_id,\n",
    "            ).to(device)\n",
    "            loss = F.ctc_loss(\n",
    "                pred, y, pred_lengths, example[\"text_int_lengths\"], blank=blank_id\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "            # loss.backward()\n",
    "            pred = pred.permute(1, 0, 2)\n",
    "            beam_results, beam_scores, timesteps, out_lens = decoder.decode(\n",
    "                pred  # TODO: , seq_lens=example[\"text_int_lengths\"]\n",
    "            )\n",
    "            for i in range(len(y)):\n",
    "                target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "                # target_text = target_text.strip()\n",
    "                target_text = target_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "                references.append(target_text)\n",
    "                if i < len(beam_results):\n",
    "                    pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                    try:\n",
    "                        pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                        # pred_text = pred_text.strip()\n",
    "                        pred_text = pred_text.replace(\n",
    "                            trainset.text_transform.pad_token, \"\"\n",
    "                        )\n",
    "                    except:\n",
    "                        print(f\"!!!ERROR!!! batch idx: {batch_i}, i: {i}\")\n",
    "                        break\n",
    "                    predictions.append(pred_text)\n",
    "            torch.cuda.empty_cache()\n",
    "        train_loss = np.mean(losses)\n",
    "        train_wer = jiwer.wer(references, predictions)\n",
    "        print(f\"Epoch {e} train loss: {train_loss}, train wer: {train_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=1,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "chars = \"\".join(testset.text_transform.chars) + \"_\"\n",
    "decoder = CTCBeamDecoder(\n",
    "    chars,\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    "    beam_width=20,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "# consine annealing scheduler\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=100)\n",
    "\n",
    "# eval mode\n",
    "# model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# loop through epochs here\n",
    "# with torch.no_grad():\n",
    "model.train()\n",
    "for e in range(10):\n",
    "    losses = []\n",
    "    references = []\n",
    "    predictions = []\n",
    "    wers = []\n",
    "    for batch_i, example in tqdm.tqdm(\n",
    "        enumerate(dataloader), \"Train step\", disable=None\n",
    "    ):\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 1000).float().to(device)\n",
    "        pred = model(X)\n",
    "        pred = F.log_softmax(pred, 2)\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths),\n",
    "            batch_first=False,\n",
    "            # padding_value = n_chars-1,\n",
    "            padding_value=trainset.text_transform.pad_token_id,\n",
    "        )\n",
    "        y = nn.utils.rnn.pad_sequence(\n",
    "            example[\"text_int\"],\n",
    "            batch_first=True,\n",
    "            # padding_value = n_chars-1,\n",
    "            padding_value=trainset.text_transform.pad_token_id,\n",
    "        ).to(device)\n",
    "        loss = F.ctc_loss(\n",
    "            pred, y, pred_lengths, example[\"text_int_lengths\"], blank=blank_id\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        pred = pred.permute(1, 0, 2)\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(\n",
    "            pred  # TODO: , seq_lens=example[\"text_int_lengths\"]\n",
    "        )\n",
    "        for i in range(len(y)):\n",
    "            target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "            # target_text = target_text.strip()\n",
    "            target_text = target_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "            references.append(target_text)\n",
    "            if i < len(beam_results):\n",
    "                pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                try:\n",
    "                    pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                    # pred_text = pred_text.strip()\n",
    "                    pred_text = pred_text.replace(trainset.text_transform.pad_token, \"\")\n",
    "                except:\n",
    "                    print(f\"!!!ERROR!!! batch idx: {batch_i}, i: {i}\")\n",
    "                    break\n",
    "                predictions.append(pred_text)\n",
    "        if (batch_i + 1) % 2 == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss = np.mean(losses)\n",
    "    train_wer = jiwer.wer(references, predictions)\n",
    "    print(f\"Epoch {e} train loss: {train_loss}, train wer: {train_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her',\n",
       " 'she found herself in a long low hall which was lit up by a row of lamps hanging from the roof nabakatokia bagabornabou cacahuamilpa abecedarian cabalistically cadaverousness lafayette chaboisseau bababalouk sabachthani abacadabra eatanswill babalatchi bababalouk hadadrimmon',\n",
       " 'it ll never do to ask perhaps i shall see it written up somewhere ekateringofsky abdalmalek dabulamanzi babalatchi alcacarquivir arbalestriers balachulish bababalouk babebibobubybaeboe babebibobubybaeboe abandonment habareskul academicianship babalatchi adachigahara academicianship jagadananda cabalistically',\n",
       " 'i hope they ll remember her saucer of milk at tea time babebibobubybaeboe babebibobubybaeboe sbarovitch babalatchi academicianship anachronistically gabardines babalatchi catachrestically bababalouk babalatchi gabrilowitsch babalatchi academicianship hadatchishi badakhshan gabardines academicianship',\n",
       " 'oh my ears and whiskers how late it s getting she was close behind it when she turned the corner but the rabbit was no longer to be seen sabachthani academicianship cabalistically labanyalekha cacahuamilpa bababalouk babebibobubybaeboe darachanarvan achaemenidae babamarishi',\n",
       " 'to be sure this generally happens when one eats caernarvonshire gabardines babalatchi cadaverousness canandaigua bababalouk bagabornabou cacahuamilpa babalatchi hadadrimmon bhamaniwallah haranguing abandonment apalachicola beaconsfield calabashes ragamuffins kabibonokka macadamization',\n",
       " 'here and there she saw maps and pictures hung up on pegs she took down a jar from one of the shelves as she passed it was labelled orange marmalade exageration beachcombers radamanthus babalatchi haberdashery rajahmahendri nabakatokia macadamization cabalistically jafanatapan',\n",
       " 'it s almost certain to disagree with you sooner or later abacadabra bababalouk babalatchi achaemenidae achaemenidae racecourses bicarbonate academicianship babalatchi academicianship darachanarvan madamoiselle cacaracamouchen cacaracamouchen haberdashery bagabornabou cabalistically babalatchi bhamaniwallah babamarishi']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her',\n",
       " 'she found herself in a long low hall which was lit up by a row of lamps hanging from the roofaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'it ll never do to ask perhaps i shall see it written up somewhereaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'i hope they ll remember her saucer of milk at tea timeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'oh my ears and whiskers how late it s getting she was close behind it when she turned the corner but the rabbit was no longer to be seenaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'to be sure this generally happens when one eats cakeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'here and there she saw maps and pictures hung up on pegs she took down a jar from one of the shelves as she passed it was labelled orange marmaladeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'it s almost certain to disagree with you sooner or lateraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=devset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=8,\n",
    ")\n",
    "n_chars = len(devset.text_transform.chars)\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "decoder = CTCBeamDecoder(\n",
    "    devset.text_transform.chars + \"_\",\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    ")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "lr_sched = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[125, 150, 175], gamma=0.5\n",
    ")\n",
    "# eval mode\n",
    "model.eval()\n",
    "# loop through epochs here\n",
    "losses = []\n",
    "references = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_i, example in tqdm.tqdm(\n",
    "        enumerate(dataloader), \"Train step\", disable=None\n",
    "    ):\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 5000).float().to(device)\n",
    "        pred = model(X)\n",
    "        pred = F.log_softmax(pred, 2)\n",
    "\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred_pad = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths), batch_first=False\n",
    "        )\n",
    "\n",
    "        y = nn.utils.rnn.pad_sequence(example[\"text_int\"], batch_first=True).to(\n",
    "            device\n",
    "        )  # TODO: padding_value\n",
    "        loss = F.ctc_loss(\n",
    "            pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        pred_pad = pred_pad.permute(1, 0, 2)\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred_pad)\n",
    "        for i in range(len(y)):\n",
    "            target_text = trainset.text_transform.int_to_text(y[i].cpu().numpy())\n",
    "            references.append(target_text)\n",
    "            if i < len(beam_results):\n",
    "                pred_int = beam_results[i, 0, : out_lens[i, 0]].tolist()\n",
    "                try:\n",
    "                    pred_text = trainset.text_transform.int_to_text(pred_int)\n",
    "                except:\n",
    "                    print(f\"batch idx: {batch_i}, i: {i}\")\n",
    "                    break\n",
    "                predictions.append(pred_text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1250, 38]), torch.Size([8, 309]), torch.Size([8, 2368, 38]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, y.shape, pred_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esemplastic babebibobubybaeboe babebibobubybaeboe icaromenippus balancing garamapingwe academicianship cabalistically achaemenidae babalatchi academicians macadamization cacaracamouchen haberdashery hadadrimmon cabalistically araucanians achaemenidae cacaracamouchen academicians achaemenidae arabesques babalatchi cacahuamilpa cabalistically academicians abhandlungen mablethorpe cabalistically babalatchi bababalouk sagaciously abhandlungen',\n",
       " 'deinde astonished eyatonkawee capabilities babalatchi bababalouk gabardines cabalistically chakamankabudibaba cadaverousness babalatchi sagaciously bagabornabou alcacarquivir tablecloths eachdaireachd gcalekaland babebibobubybaeboe cabalistically dabulamanzi bagabornabou academicianship',\n",
       " 'ebenezer gablehurst falcinellus cabalistically anabaptists jadakweniyosaon capabilities damanarkist cacahuamilpa babebibobubybaeboe lagadigadeou jadakweniyosaon bibativeness babalatchi anabaptists afanassievna eachdaireachd nayakoghstonde adachigahara adachigahara cachemirian academicians babebibobubybaeboe tanacharisson bhagavadgita lagadigadeou babalatchi babalatchi cabalistically blagadenztoiga bagabornabou',\n",
       " 'rhodomont elsie in someat the esterel are neater elapsed even therese seated oletta let entendait',\n",
       " 'i loewestein hareton elephantiasis cacaracamouchen abhandlungen cabalistically labanyalekha beachcombers achaemenidae babalatchi bagabornabou babalatchi kaiakahinalii nabakatokia hackelberend dabulamanzi cacahuamilpa gabardines cadaverousness academicians beachcombers bababalouk macadamization cacaracamouchen albatrosses cacahuamilpa hadadrimmon',\n",
       " 'einerhalben hadadrimmon academicians jadakweniyosaon cadaverousness fabrications eavesdropping academicianship cabalistically academicians adachigahara cabalistically babalatchi labanyalekha hamadryades bababalouk macadamization camaralzaman cabalistically abandonment achaemenidae beachcombers badakhshan macadamization eachdaireachd hadatchishi macadamization sabachthani babebibobubybaeboe',\n",
       " 'eternities gabardines academicians cabalistically academicians bababalouk dabulamanzi academicianship damanarkist academicians madamoiselle cabalistically babebibobubybaeboe bamangwato adachigahara bababalouk tabachetti cabalistically adachigahara babebibobubybaeboe cabalistically cabalistically academicians bagabornabou academicians cacaracamouchen gabardines cabalistically babamarishi',\n",
       " 'e u t is consoled nonappearance neither a tsernoyevitcha academicianship academicianship cacahuamilpa babalatchi jadakweniyosaon cacaracamouchen babalatchi cabalistically adairsville bababalouk cacaracamouchen bababalouk cacahuamilpa bababalouk']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must be getting somewhere near the center of the earthaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'either the well was very deep or she fell very slowly for she had plenty of time as she went down to look about her and to wonder what was going to happen nextaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'please maam is this new zealand or australia and she tried to curtsey as she spoke fancy curtseying as you re falling through the air do you think you could manage it and what an ignorant little girl she ll think me for asking noaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'the poor little thing sat down and cried come there s no use crying like that said alice to herself rather sharply i advise you to leave off this minute she generally gave herself very good advice though she very seldom followed it and sometimes she scolded herself so severely as to bring tears into her eyes',\n",
       " 'she felt that she was dozing off and had just begun to dream that she was walking hand in hand with dinahaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'and the white rabbit was still in sight hurrying downaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'she felt a little nervous about this for it might end you knowaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'the rabbit hole went straight on like a tunnel for some way and then dipped suddenly down so suddenly that alice had not a moment to think about stopping herself before she found herself falling down a very deep wellaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeer theeeeereeeeeeeeeeeee was nothing so veeeeeeeeeeeeeeeeeeeeeeeeeery reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeemarrkableeeeeeeeeeeeeeeeeeeeeeeeeeeee in that nor did aliceeeeeeeeeeeeeeeeeeeeeee think it  so veery much  out of theeeeeeeeeeeeee way  to hear the rab_bit say to itself oh  deeeeeeeeeeeear oh deeeeeeeeeeeeeeeeeeeeeeeeeeeear i sshal_l beeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee       latteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee wheeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeen   sheeeeeeeeeeeeeeeeeeeeeeeeeeeee thought it oveeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeer afteeeeeeeeeeeeeeeeeeeeerwards it oc_cur_reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeed to heeeeeeeeeeeeeeeeeeeeeeeer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transform = TextTransform()\n",
    "text_transform.chars += \"_\"\n",
    "text_transform.int_to_text(p1.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it ove'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)\n",
    "pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()\n",
    "pred_text = testset.text_transform.int_to_text(pred_int)\n",
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 1250])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13043478260869565"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_txt = testset.text_transform.int_to_text(y[0].cpu().numpy())\n",
    "jiwer.wer(target_txt, pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "batch_size = 2\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=0,\n",
    "    collate_fn=EEGDataset.collate_raw,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "blank_id = len(testset.text_transform.chars)\n",
    "decoder = CTCBeamDecoder(\n",
    "    devset.text_transform.chars + \"_\",\n",
    "    blank_id=blank_id,\n",
    "    log_probs_input=True,\n",
    "    model_path=\"lm.binary\",\n",
    "    alpha=1.5,\n",
    "    beta=1.85,\n",
    ")\n",
    "model.eval()\n",
    "references = []\n",
    "predictions = []\n",
    "losses = []\n",
    "with torch.no_grad():\n",
    "    for example in tqdm.tqdm(dataloader, \"Evaluate\", disable=None):\n",
    "        # X = example[\"eeg_raw\"][0].float().to(device)\n",
    "        X = combine_fixed_length(example[\"eeg_raw\"], 5000).float().to(device)\n",
    "        pred = F.log_softmax(model(X), -1)\n",
    "        pred_lengths = [l // 4 for l in example[\"lengths\"]]\n",
    "        pred_pad = nn.utils.rnn.pad_sequence(\n",
    "            decollate_tensor(pred, pred_lengths), batch_first=False\n",
    "        )\n",
    "\n",
    "        # y = nn.utils.rnn.pad_sequence(example[\"text_int\"], batch_first=True).to(device)\n",
    "        y = torch.cat(example[\"text_int\"]).to(device)\n",
    "        loss = F.ctc_loss(\n",
    "            pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)\n",
    "        pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()\n",
    "\n",
    "        pred_text = testset.text_transform.int_to_text(pred_int)\n",
    "        target_text = testset.text_transform.clean_text(example[\"labels\"][0])\n",
    "\n",
    "        references.append(target_text)\n",
    "        predictions.append(pred_text)\n",
    "wer = jiwer.wer(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_pad: torch.Size([761, 2, 38])\n",
      "pred.shape: torch.Size([1, 1250, 38])\n",
      "y.shape: torch.Size([155])\n",
      "pred_lengths: [248, 761]\n",
      "n_chars: 37\n",
      "loss: 2.801051139831543\n"
     ]
    }
   ],
   "source": [
    "print(f\"pred_pad: {pred_pad.shape}\")\n",
    "print(f\"pred.shape: {pred.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"pred_lengths: {pred_lengths}\")\n",
    "print(f\"n_chars: {n_chars}\")\n",
    "loss = F.ctc_loss(pred_pad, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars)\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetota',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetaheite',\n",
       " 'tetootne',\n",
       " 'teetotalis',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetota',\n",
       " 'teetota',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetotale',\n",
       " 'tetootne',\n",
       " 'tetootne',\n",
       " 'teetotalis',\n",
       " 'etteniot',\n",
       " 'tetootne',\n",
       " 'teetotale',\n",
       " 'etonensis',\n",
       " 'tetootne',\n",
       " 'tottontai']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37]\n"
     ]
    }
   ],
   "source": [
    "pred_exp = torch.exp(pred)  # Convert log_probs back to probabilities\n",
    "pred_max = pred_exp.argmax(dim=-1)  # Get the most likely token at each time step\n",
    "print(pred_max.squeeze().tolist())  # Inspect the token indices over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred_pad)\n",
    "beam_results_o, beam_scores_o, timesteps_o, out_lens_o = decoder.decode(pred)\n",
    "# pred_int = beam_results[0, 0, : out_lens[0, 0]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([761, 100, 1]), torch.Size([1, 100, 1250]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape, beam_results_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1250, 38])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_int = beam_results_o[0, 0, : out_lens_o[:, 0]].tolist()  # out_lens_o[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset.text_transform.int_to_text(pred_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([761, 100, 1]), 120)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_results.shape, len(example[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8002, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.ctc_loss(pred_prd, y, pred_lengths, example[\"text_int_lengths\"], blank=n_chars)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eeg_raw', 'labels', 'lengths', 'text_int', 'text_int_lengths'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tetootne'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789 '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.text_transform.chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset.text_transform.chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_architecture import EEGModel\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from data_utils import combine_fixed_length, decollate_tensor\n",
    "from IPython.core.debugger import Pdb\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "l2 = 1e-5\n",
    "n_epochs = 2\n",
    "learning_rate_warmup = 100\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(devset, batch_size=1)\n",
    "for example in tqdm.tqdm(dataloader, \"Evaluate\", disable=None):\n",
    "    target = example[\"label\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MUST BE GETTING SOMEWHERE NEAR THE CENTER OF THE EARTH'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    }
   ],
   "source": [
    "ds_brennan = BrennanDataset(\n",
    "    root_dir=base_dir,\n",
    "    phoneme_dir=base_dir / \"phonemes\",\n",
    "    idx=\"S04\",\n",
    "    phoneme_dict_path=base_dir / \"phoneme_dict.txt\",\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Alice\n",
      "dict_keys(['label', 'audio_feats', 'audio_raw', 'eeg_raw', 'eeg_feats', 'phonemes'])\n",
      "(104, 128) (159, 300) 104\n"
     ]
    }
   ],
   "source": [
    "item2 = ds_brennan[0]\n",
    "print(item2.keys())\n",
    "print(item2[\"audio_feats\"].shape, item2[\"eeg_feats\"].shape, len(item2[\"phonemes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /ocean/projects/cis240129p/shared/data/eeg_alice/S04.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 368449  =      0.000 ...   736.898 secs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1703, Test dataset length: 426\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = create_datasets(subjects_used, base_dir)\n",
    "\n",
    "print(\n",
    "    f\"Train dataset length: {len(train_dataset)}, Test dataset length: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     A custom collate function that handles different types of data in a batch.\n",
    "#     It dynamically creates batches by converting arrays or lists to tensors and\n",
    "#     applies padding to variable-length sequences.\n",
    "#     \"\"\"\n",
    "#     batch_dict = {}\n",
    "#     for key in batch[0].keys():\n",
    "#         batch_items = [item[key] for item in batch]\n",
    "#         if isinstance(batch_items[0], np.ndarray) or isinstance(\n",
    "#             batch_items[0], torch.Tensor\n",
    "#         ):\n",
    "#             if isinstance(batch_items[0], np.ndarray):\n",
    "#                 batch_items = [torch.tensor(b) for b in batch_items]\n",
    "#             if len(batch_items[0].shape) > 0:\n",
    "#                 batch_dict[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "#                     batch_items, batch_first=True  # pad with zeros\n",
    "#                 )\n",
    "#             else:\n",
    "#                 batch_dict[key] = torch.stack(batch_items)\n",
    "#         else:\n",
    "#             batch_dict[key] = batch_items\n",
    "\n",
    "#     return batch_dict\n",
    "\n",
    "\n",
    "train_dataloder = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label <class 'str'>\n",
      "audio_feats (104, 128) <class 'numpy.ndarray'>\n",
      "audio_raw (16735,) <class 'numpy.ndarray'>\n",
      "eeg_raw (520, 62) <class 'numpy.ndarray'>\n",
      "eeg_feats (159, 310) <class 'numpy.ndarray'>\n",
      "phonemes (104,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "for k, v in item.items():\n",
    "    try:\n",
    "        print(k, v.shape, type(v))\n",
    "    except:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"eeg_raw\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "1\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "2\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "3\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n",
      "4\n",
      "label <class 'list'>\n",
      "audio_feats torch.Size([2, 130, 128]) <class 'torch.Tensor'>\n",
      "audio_raw torch.Size([2, 20800]) <class 'torch.Tensor'>\n",
      "eeg_raw torch.Size([2, 520, 62]) <class 'torch.Tensor'>\n",
      "eeg_feats torch.Size([2, 159, 310]) <class 'torch.Tensor'>\n",
      "phonemes torch.Size([2, 130]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "i = 0\n",
    "for batch in train_dataloder:\n",
    "    print(i)\n",
    "    for k, v in batch.items():\n",
    "        try:\n",
    "            print(k, v.shape, type(v))\n",
    "        except:\n",
    "            print(k, type(v))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
